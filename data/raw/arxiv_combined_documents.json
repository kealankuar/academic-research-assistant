[
  {
    "title": "Opening the black box of deep learning",
    "text": "Opening the black box of deep learning. The great success of deep learning shows that its technology contains\nprofound truth, and understanding its internal mechanism not only has important\nimplications for the development of its technology and effective application in\nvarious fields, but also provides meaningful insights into the understanding of\nhuman brain mechanism. At present, most of the theoretical research on deep\nlearning is based on mathematics. This dissertation proposes that the neural\nnetwork of deep learning is a physical system, examines deep learning from\nthree different perspectives: microscopic, macroscopic, and physical world\nviews, answers multiple theoretical puzzles in deep learning by using physics\nprinciples. For example, from the perspective of quantum mechanics and\nstatistical physics, this dissertation presents the calculation methods for\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\nMachine, as well as the selection of cost functions, explains why deep learning\nmust be deep, what characteristics are learned in deep learning, why\nConvolutional Neural Networks do not have to be trained layer by layer, and the\nlimitations of deep learning, etc., and proposes the theoretical direction and\nbasis for the further development of deep learning now and in the future. The\nbrilliance of physics flashes in deep learning, we try to establish the deep\nlearning technology based on the scientific theory of physics.",
    "pdf_link": "http://arxiv.org/pdf/1805.08355v1.pdf",
    "authors": [
      "Dian Lei",
      "Xiaoxiao Chen",
      "Jianfei Zhao"
    ]
  },
  {
    "title": "Deep learning research landscape & roadmap in a nutshell: past, present\n  and future -- Towards deep cortical learning",
    "text": "Deep learning research landscape & roadmap in a nutshell: past, present\n  and future -- Towards deep cortical learning. The past, present and future of deep learning is presented in this work.\nGiven this landscape & roadmap, we predict that deep cortical learning will be\nthe convergence of deep learning & cortical learning which builds an artificial\ncortical column ultimately.",
    "pdf_link": "http://arxiv.org/pdf/1908.02130v1.pdf",
    "authors": [
      "Aras R. Dargazany"
    ]
  },
  {
    "title": "Concept-Oriented Deep Learning",
    "text": "Concept-Oriented Deep Learning. Concepts are the foundation of human deep learning, understanding, and\nknowledge integration and transfer. We propose concept-oriented deep learning\n(CODL) which extends (machine) deep learning with concept representations and\nconceptual understanding capability. CODL addresses some of the major\nlimitations of deep learning: interpretability, transferability, contextual\nadaptation, and requirement for lots of labeled training data. We discuss the\nmajor aspects of CODL including concept graph, concept representations, concept\nexemplars, and concept representation learning systems supporting incremental\nand continual learning.",
    "pdf_link": "http://arxiv.org/pdf/1806.01756v1.pdf",
    "authors": [
      "Daniel T Chang"
    ]
  },
  {
    "title": "A First Look at Deep Learning Apps on Smartphones",
    "text": "A First Look at Deep Learning Apps on Smartphones. We are in the dawn of deep learning explosion for smartphones. To bridge the\ngap between research and practice, we present the first empirical study on\n16,500 the most popular Android apps, demystifying how smartphone apps exploit\ndeep learning in the wild. To this end, we build a new static tool that\ndissects apps and analyzes their deep learning functions. Our study answers\nthreefold questions: what are the early adopter apps of deep learning, what do\nthey use deep learning for, and how do their deep learning models look like.\nOur study has strong implications for app developers, smartphone vendors, and\ndeep learning R\\&D. On one hand, our findings paint a promising picture of deep\nlearning for smartphones, showing the prosperity of mobile deep learning\nframeworks as well as the prosperity of apps building their cores atop deep\nlearning. On the other hand, our findings urge optimizations on deep learning\nmodels deployed on smartphones, the protection of these models, and validation\nof research ideas on these models.",
    "pdf_link": "http://arxiv.org/pdf/1812.05448v4.pdf",
    "authors": [
      "Mengwei Xu",
      "Jiawei Liu",
      "Yuanqiang Liu",
      "Felix Xiaozhu Lin",
      "Yunxin Liu",
      "Xuanzhe Liu"
    ]
  },
  {
    "title": "Geometrization of deep networks for the interpretability of deep\n  learning systems",
    "text": "Geometrization of deep networks for the interpretability of deep\n  learning systems. How to understand deep learning systems remains an open problem. In this\npaper we propose that the answer may lie in the geometrization of deep\nnetworks. Geometrization is a bridge to connect physics, geometry, deep network\nand quantum computation and this may result in a new scheme to reveal the rule\nof the physical world. By comparing the geometry of image matching and deep\nnetworks, we show that geometrization of deep networks can be used to\nunderstand existing deep learning systems and it may also help to solve the\ninterpretability problem of deep learning systems.",
    "pdf_link": "http://arxiv.org/pdf/1901.02354v2.pdf",
    "authors": [
      "Xiao Dong",
      "Ling Zhou"
    ]
  },
  {
    "title": "Why & When Deep Learning Works: Looking Inside Deep Learnings",
    "text": "Why & When Deep Learning Works: Looking Inside Deep Learnings. The Intel Collaborative Research Institute for Computational Intelligence\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\nLearning researchers to address the challenge of \"Why & When Deep Learning\nworks\", with the goal of looking inside Deep Learning, providing insights on\nhow deep networks function, and uncovering key observations on their\nexpressiveness, limitations, and potential. The output of this challenge\nresulted in five papers that address different facets of deep learning. These\ndifferent facets include a high-level understating of why and when deep\nnetworks work (and do not work), the impact of geometry on the expressiveness\nof deep networks, and making deep networks interpretable.",
    "pdf_link": "http://arxiv.org/pdf/1705.03921v1.pdf",
    "authors": [
      "Ronny Ronen"
    ]
  },
  {
    "title": "Learning Task-aware Robust Deep Learning Systems",
    "text": "Learning Task-aware Robust Deep Learning Systems. Many works demonstrate that deep learning system is vulnerable to adversarial\nattack. A deep learning system consists of two parts: the deep learning task\nand the deep model. Nowadays, most existing works investigate the impact of the\ndeep model on robustness of deep learning systems, ignoring the impact of the\nlearning task. In this paper, we adopt the binary and interval label encoding\nstrategy to redefine the classification task and design corresponding loss to\nimprove robustness of the deep learning system. Our method can be viewed as\nimproving the robustness of deep learning systems from both the learning task\nand deep model. Experimental results demonstrate that our learning task-aware\nmethod is much more robust than traditional classification while retaining the\naccuracy.",
    "pdf_link": "http://arxiv.org/pdf/2010.05125v2.pdf",
    "authors": [
      "Keji Han",
      "Yun Li",
      "Xianzhong Long",
      "Yao Ge"
    ]
  },
  {
    "title": "Deep Learning in Software Engineering",
    "text": "Deep Learning in Software Engineering. Recent years, deep learning is increasingly prevalent in the field of\nSoftware Engineering (SE). However, many open issues still remain to be\ninvestigated. How do researchers integrate deep learning into SE problems?\nWhich SE phases are facilitated by deep learning? Do practitioners benefit from\ndeep learning? The answers help practitioners and researchers develop practical\ndeep learning models for SE tasks. To answer these questions, we conduct a\nbibliography analysis on 98 research papers in SE that use deep learning\ntechniques. We find that 41 SE tasks in all SE phases have been facilitated by\ndeep learning integrated solutions. In which, 84.7% papers only use standard\ndeep learning models and their variants to solve SE problems. The\npracticability becomes a concern in utilizing deep learning techniques. How to\nimprove the effectiveness, efficiency, understandability, and testability of\ndeep learning based solutions may attract more SE researchers in the future.",
    "pdf_link": "http://arxiv.org/pdf/1805.04825v1.pdf",
    "authors": [
      "Xiaochen Li",
      "He Jiang",
      "Zhilei Ren",
      "Ge Li",
      "Jingxuan Zhang"
    ]
  },
  {
    "title": "Moving Deep Learning into Web Browser: How Far Can We Go?",
    "text": "Moving Deep Learning into Web Browser: How Far Can We Go?. Recently, several JavaScript-based deep learning frameworks have emerged,\nmaking it possible to perform deep learning tasks directly in browsers.\nHowever, little is known on what and how well we can do with these frameworks\nfor deep learning in browsers. To bridge the knowledge gap, in this paper, we\nconduct the first empirical study of deep learning in browsers. We survey 7\nmost popular JavaScript-based deep learning frameworks, investigating to what\nextent deep learning tasks have been supported in browsers so far. Then we\nmeasure the performance of different frameworks when running different deep\nlearning tasks. Finally, we dig out the performance gap between deep learning\nin browsers and on native platforms by comparing the performance of\nTensorFlow.js and TensorFlow in Python. Our findings could help application\ndevelopers, deep-learning framework vendors and browser vendors to improve the\nefficiency of deep learning in browsers.",
    "pdf_link": "http://arxiv.org/pdf/1901.09388v2.pdf",
    "authors": [
      "Yun Ma",
      "Dongwei Xiang",
      "Shuyu Zheng",
      "Deyu Tian",
      "Xuanzhe Liu"
    ]
  },
  {
    "title": "Greedy Deep Dictionary Learning",
    "text": "Greedy Deep Dictionary Learning. In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.",
    "pdf_link": "http://arxiv.org/pdf/1602.00203v1.pdf",
    "authors": [
      "Snigdha Tariyal",
      "Angshul Majumdar",
      "Richa Singh",
      "Mayank Vatsa"
    ]
  },
  {
    "title": "Quantum Neural Networks: Concepts, Applications, and Challenges",
    "text": "Quantum Neural Networks: Concepts, Applications, and Challenges. Quantum deep learning is a research field for the use of quantum computing\ntechniques for training deep neural networks. The research topics and\ndirections of deep learning and quantum computing have been separated for long\ntime, however by discovering that quantum circuits can act like artificial\nneural networks, quantum deep learning research is widely adopted. This paper\nexplains the backgrounds and basic principles of quantum deep learning and also\nintroduces major achievements. After that, this paper discusses the challenges\nof quantum deep learning research in multiple perspectives. Lastly, this paper\npresents various future research directions and application fields of quantum\ndeep learning.",
    "pdf_link": "http://arxiv.org/pdf/2108.01468v1.pdf",
    "authors": [
      "Yunseok Kwak",
      "Won Joon Yun",
      "Soyi Jung",
      "Joongheon Kim"
    ]
  },
  {
    "title": "NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders\n  of Deep Giants",
    "text": "NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders\n  of Deep Giants. Tiny deep learning has attracted increasing attention driven by the\nsubstantial demand for deploying deep learning on numerous intelligent\nInternet-of-Things devices. However, it is still challenging to unleash tiny\ndeep learning's full potential on both large-scale datasets and downstream\ntasks due to the under-fitting issues caused by the limited model capacity of\ntiny neural networks (TNNs). To this end, we propose a framework called\nNetBooster to empower tiny deep learning by augmenting the architectures of\nTNNs via an expansion-then-contraction strategy. Extensive experiments show\nthat NetBooster consistently outperforms state-of-the-art tiny deep learning\nsolutions.",
    "pdf_link": "http://arxiv.org/pdf/2306.13586v1.pdf",
    "authors": [
      "Zhongzhi Yu",
      "Yonggan Fu",
      "Jiayi Yuan",
      "Haoran You",
      "Yingyan Lin"
    ]
  },
  {
    "title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
    "text": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey. Deep reinforcement learning augments the reinforcement learning framework and\nutilizes the powerful representation of deep neural networks. Recent works have\ndemonstrated the remarkable successes of deep reinforcement learning in various\ndomains including finance, medicine, healthcare, video games, robotics, and\ncomputer vision. In this work, we provide a detailed review of recent and\nstate-of-the-art research advances of deep reinforcement learning in computer\nvision. We start with comprehending the theories of deep learning,\nreinforcement learning, and deep reinforcement learning. We then propose a\ncategorization of deep reinforcement learning methodologies and discuss their\nadvantages and limitations. In particular, we divide deep reinforcement\nlearning into seven main categories according to their applications in computer\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\nthese categories is further analyzed with reinforcement learning techniques,\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\nof the existing publicly available datasets and examine source code\navailability. Finally, we present some open issues and discuss future research\ndirections on deep reinforcement learning in computer vision",
    "pdf_link": "http://arxiv.org/pdf/2108.11510v1.pdf",
    "authors": [
      "Ngan Le",
      "Vidhiwar Singh Rathour",
      "Kashu Yamazaki",
      "Khoa Luu",
      "Marios Savvides"
    ]
  },
  {
    "title": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep\n  Probabilistic Models",
    "text": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep\n  Probabilistic Models. Probabilistic deep learning is deep learning that accounts for uncertainty,\nboth model uncertainty and data uncertainty. It is based on the use of\nprobabilistic models and deep neural networks. We distinguish two approaches to\nprobabilistic deep learning: probabilistic neural networks and deep\nprobabilistic models. The former employs deep neural networks that utilize\nprobabilistic layers which can represent and process uncertainty; the latter\nuses probabilistic models that incorporate deep neural network components which\ncapture complex non-linear stochastic relationships between the random\nvariables. We discuss some major examples of each approach including Bayesian\nneural networks and mixture density networks (for probabilistic neural\nnetworks), and variational autoencoders, deep Gaussian processes and deep mixed\neffects models (for deep probabilistic models). TensorFlow Probability is a\nlibrary for probabilistic modeling and inference which can be used for both\napproaches of probabilistic deep learning. We include its code examples for\nillustration.",
    "pdf_link": "http://arxiv.org/pdf/2106.00120v3.pdf",
    "authors": [
      "Daniel T. Chang"
    ]
  },
  {
    "title": "Towards energy-efficient Deep Learning: An overview of energy-efficient\n  approaches along the Deep Learning Lifecycle",
    "text": "Towards energy-efficient Deep Learning: An overview of energy-efficient\n  approaches along the Deep Learning Lifecycle. Deep Learning has enabled many advances in machine learning applications in\nthe last few years. However, since current Deep Learning algorithms require\nmuch energy for computations, there are growing concerns about the associated\nenvironmental costs. Energy-efficient Deep Learning has received much attention\nfrom researchers and has already made much progress in the last couple of\nyears. This paper aims to gather information about these advances from the\nliterature and show how and at which points along the lifecycle of Deep\nLearning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation)\nit is possible to reduce energy consumption.",
    "pdf_link": "http://arxiv.org/pdf/2303.01980v1.pdf",
    "authors": [
      "Vanessa Mehlin",
      "Sigurd Schacht",
      "Carsten Lanquillon"
    ]
  },
  {
    "title": "A Unified Framework of Deep Neural Networks by Capsules",
    "text": "A Unified Framework of Deep Neural Networks by Capsules. With the growth of deep learning, how to describe deep neural networks\nunifiedly is becoming an important issue. We first formalize neural networks\nmathematically with their directed graph representations, and prove a\ngeneration theorem about the induced networks of connected directed acyclic\ngraphs. Then, we set up a unified framework for deep learning with capsule\nnetworks. This capsule framework could simplify the description of existing\ndeep neural networks, and provide a theoretical basis of graphic designing and\nprogramming techniques for deep learning models, thus would be of great\nsignificance to the advancement of deep learning.",
    "pdf_link": "http://arxiv.org/pdf/1805.03551v2.pdf",
    "authors": [
      "Yujian Li",
      "Chuanhui Shan"
    ]
  },
  {
    "title": "Integrating Learning and Reasoning with Deep Logic Models",
    "text": "Integrating Learning and Reasoning with Deep Logic Models. Deep learning is very effective at jointly learning feature representations\nand classification models, especially when dealing with high dimensional input\npatterns. Probabilistic logic reasoning, on the other hand, is capable to take\nconsistent and robust decisions in complex environments. The integration of\ndeep learning and logic reasoning is still an open-research problem and it is\nconsidered to be the key for the development of real intelligent agents. This\npaper presents Deep Logic Models, which are deep graphical models integrating\ndeep learning and logic reasoning both for learning and inference. Deep Logic\nModels create an end-to-end differentiable architecture, where deep learners\nare embedded into a network implementing a continuous relaxation of the logic\nknowledge. The learning process allows to jointly learn the weights of the deep\nlearners and the meta-parameters controlling the high-level reasoning. The\nexperimental results show that the proposed methodology overtakes the\nlimitations of the other approaches that have been proposed to bridge deep\nlearning and reasoning.",
    "pdf_link": "http://arxiv.org/pdf/1901.04195v1.pdf",
    "authors": [
      "Giuseppe Marra",
      "Francesco Giannini",
      "Michelangelo Diligenti",
      "Marco Gori"
    ]
  },
  {
    "title": "Deep Learning in the Field of Biometric Template Protection: An Overview",
    "text": "Deep Learning in the Field of Biometric Template Protection: An Overview. Today, deep learning represents the most popular and successful form of\nmachine learning. Deep learning has revolutionised the field of pattern\nrecognition, including biometric recognition. Biometric systems utilising deep\nlearning have been shown to achieve auspicious recognition accuracy, surpassing\nhuman performance. Apart from said breakthrough advances in terms of biometric\nperformance, the use of deep learning was reported to impact different\ncovariates of biometrics such as algorithmic fairness, vulnerability to\nattacks, or template protection. Technologies of biometric template protection\nare designed to enable a secure and privacy-preserving deployment of\nbiometrics. In the recent past, deep learning techniques have been frequently\napplied in biometric template protection systems for various purposes. This\nwork provides an overview of how advances in deep learning take influence on\nthe field of biometric template protection. The interrelation between improved\nbiometric performance rates and security in biometric template protection is\nelaborated. Further, the use of deep learning for obtaining feature\nrepresentations that are suitable for biometric template protection is\ndiscussed. Novel methods that apply deep learning to achieve various goals of\nbiometric template protection are surveyed along with deep learning-based\nattacks.",
    "pdf_link": "http://arxiv.org/pdf/2303.02715v1.pdf",
    "authors": [
      "Christian Rathgeb",
      "Jascha Kolberg",
      "Andreas Uhl",
      "Christoph Busch"
    ]
  },
  {
    "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
    "text": "A Survey Analyzing Generalization in Deep Reinforcement Learning. Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto large language models, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will formalize and analyze generalization in deep\nreinforcement learning. We will explain the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\ngeneralization capabilities. Furthermore, we will categorize and explain the\nmanifold solution approaches to increase generalization, and overcome\noverfitting in deep reinforcement learning policies. From exploration to\nadversarial analysis and from regularization to robustness our paper provides\nan analysis on a wide range of subfields within deep reinforcement learning\nwith a broad scope and in-depth view. We believe our study can provide a\ncompact guideline for the current advancements in deep reinforcement learning,\nand help to construct robust deep neural policies with higher generalization\nskills.",
    "pdf_link": "http://arxiv.org/pdf/2401.02349v2.pdf",
    "authors": [
      "Ezgi Korkmaz"
    ]
  },
  {
    "title": "Transferability in Deep Learning: A Survey",
    "text": "Transferability in Deep Learning: A Survey. The success of deep learning algorithms generally depends on large-scale\ndata, while humans appear to have inherent ability of knowledge transfer, by\nrecognizing and applying relevant knowledge from previous learning experiences\nwhen encountering and solving unseen tasks. Such an ability to acquire and\nreuse knowledge is known as transferability in deep learning. It has formed the\nlong-term quest towards making deep learning as data-efficient as human\nlearning, and has been motivating fruitful design of more powerful deep\nlearning algorithms. We present this survey to connect different isolated areas\nin deep learning with their relation to transferability, and to provide a\nunified and complete view to investigating transferability through the whole\nlifecycle of deep learning. The survey elaborates the fundamental goals and\nchallenges in parallel with the core principles and methods, covering recent\ncornerstones in deep architectures, pre-training, task adaptation and domain\nadaptation. This highlights unanswered questions on the appropriate objectives\nfor learning transferable knowledge and for adapting the knowledge to new tasks\nand domains, avoiding catastrophic forgetting and negative transfer. Finally,\nwe implement a benchmark and an open-source library, enabling a fair evaluation\nof deep learning methods in terms of transferability.",
    "pdf_link": "http://arxiv.org/pdf/2201.05867v1.pdf",
    "authors": [
      "Junguang Jiang",
      "Yang Shu",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  {
    "title": "What Really is Deep Learning Doing?",
    "text": "What Really is Deep Learning Doing?. Deep learning has achieved a great success in many areas, from computer\nvision to natural language processing, to game playing, and much more. Yet,\nwhat deep learning is really doing is still an open question. There are a lot\nof works in this direction. For example, [5] tried to explain deep learning by\ngroup renormalization, and [6] tried to explain deep learning from the view of\nfunctional approximation. In order to address this very crucial question, here\nwe see deep learning from perspective of mechanical learning and learning\nmachine (see [1], [2]). From this particular angle, we can see deep learning\nmuch better and answer with confidence: What deep learning is really doing? why\nit works well, how it works, and how much data is necessary for learning. We\nalso will discuss advantages and disadvantages of deep learning at the end of\nthis work.",
    "pdf_link": "http://arxiv.org/pdf/1711.03577v1.pdf",
    "authors": [
      "Chuyu Xiong"
    ]
  },
  {
    "title": "Feature versus Raw Sequence: Deep Learning Comparative Study on\n  Predicting Pre-miRNA",
    "text": "Feature versus Raw Sequence: Deep Learning Comparative Study on\n  Predicting Pre-miRNA. Should we input known genome sequence features or input sequence itself in\ndeep learning framework? As deep learning more popular in various applications,\nresearchers often come to question whether to generate features or use raw\nsequences for deep learning. To answer this question, we study the prediction\naccuracy of precursor miRNA prediction of feature-based deep belief network and\nsequence-based convolution neural network. Tested on a variant of six-layer\nconvolution neural net and three-layer deep belief network, we find the raw\nsequence input based convolution neural network model performs similar or\nslightly better than feature based deep belief networks with best accuracy\nvalues of 0.995 and 0.990, respectively. Both the models outperform existing\nbenchmarks models. The results shows us that if provided large enough data,\nwell devised raw sequence based deep learning models can replace feature based\ndeep learning models. However, construction of well behaved deep learning model\ncan be very challenging. In cased features can be easily extracted,\nfeature-based deep learning models may be a better alternative.",
    "pdf_link": "http://arxiv.org/pdf/1710.06798v1.pdf",
    "authors": [
      "Jaya Thomas",
      "Sonia Thomas",
      "Lee Sael"
    ]
  },
  {
    "title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player\n  Multi-Agent Learning Toolbox",
    "text": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player\n  Multi-Agent Learning Toolbox. With the breakthrough of AlphaGo, deep reinforcement learning becomes a\nrecognized technique for solving sequential decision-making problems. Despite\nits reputation, data inefficiency caused by its trial and error learning\nmechanism makes deep reinforcement learning hard to be practical in a wide\nrange of areas. Plenty of methods have been developed for sample efficient deep\nreinforcement learning, such as environment modeling, experience transfer, and\ndistributed modifications, amongst which, distributed deep reinforcement\nlearning has shown its potential in various applications, such as\nhuman-computer gaming, and intelligent transportation. In this paper, we\nconclude the state of this exciting field, by comparing the classical\ndistributed deep reinforcement learning methods, and studying important\ncomponents to achieve efficient distributed learning, covering single player\nsingle agent distributed deep reinforcement learning to the most complex\nmultiple players multiple agents distributed deep reinforcement learning.\nFurthermore, we review recently released toolboxes that help to realize\ndistributed deep reinforcement learning without many modifications of their\nnon-distributed versions. By analyzing their strengths and weaknesses, a\nmulti-player multi-agent distributed deep reinforcement learning toolbox is\ndeveloped and released, which is further validated on Wargame, a complex\nenvironment, showing usability of the proposed toolbox for multiple players and\nmultiple agents distributed deep reinforcement learning under complex games.\nFinally, we try to point out challenges and future trends, hoping this brief\nreview can provide a guide or a spark for researchers who are interested in\ndistributed deep reinforcement learning.",
    "pdf_link": "http://arxiv.org/pdf/2212.00253v1.pdf",
    "authors": [
      "Qiyue Yin",
      "Tongtong Yu",
      "Shengqi Shen",
      "Jun Yang",
      "Meijing Zhao",
      "Kaiqi Huang",
      "Bin Liang",
      "Liang Wang"
    ]
  },
  {
    "title": "Are Efficient Deep Representations Learnable?",
    "text": "Are Efficient Deep Representations Learnable?. Many theories of deep learning have shown that a deep network can require\ndramatically fewer resources to represent a given function compared to a\nshallow network. But a question remains: can these efficient representations be\nlearned using current deep learning techniques? In this work, we test whether\nstandard deep learning methods can in fact find the efficient representations\nposited by several theories of deep representation. Specifically, we train deep\nneural networks to learn two simple functions with known efficient solutions:\nthe parity function and the fast Fourier transform. We find that using\ngradient-based optimization, a deep network does not learn the parity function,\nunless initialized very close to a hand-coded exact solution. We also find that\na deep linear neural network does not learn the fast Fourier transform, even in\nthe best-case scenario of infinite training data, unless the weights are\ninitialized very close to the exact hand-coded solution. Our results suggest\nthat not every element of the class of compositional functions can be learned\nefficiently by a deep network, and further restrictions are necessary to\nunderstand what functions are both efficiently representable and learnable.",
    "pdf_link": "http://arxiv.org/pdf/1807.06399v1.pdf",
    "authors": [
      "Maxwell Nye",
      "Andrew Saxe"
    ]
  },
  {
    "title": "Deep Learning: A Critical Appraisal",
    "text": "Deep Learning: A Critical Appraisal. Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.",
    "pdf_link": "http://arxiv.org/pdf/1801.00631v1.pdf",
    "authors": [
      "Gary Marcus"
    ]
  },
  {
    "title": "Deep Learning for Sentiment Analysis : A Survey",
    "text": "Deep Learning for Sentiment Analysis : A Survey. Deep learning has emerged as a powerful machine learning technique that\nlearns multiple layers of representations or features of the data and produces\nstate-of-the-art prediction results. Along with the success of deep learning in\nmany other application domains, deep learning is also popularly used in\nsentiment analysis in recent years. This paper first gives an overview of deep\nlearning and then provides a comprehensive survey of its current applications\nin sentiment analysis.",
    "pdf_link": "http://arxiv.org/pdf/1801.07883v2.pdf",
    "authors": [
      "Lei Zhang",
      "Shuai Wang",
      "Bing Liu"
    ]
  },
  {
    "title": "Deep Learning for Visual Navigation of Underwater Robots",
    "text": "Deep Learning for Visual Navigation of Underwater Robots. This paper aims to briefly survey deep learning methods for visual navigation\nof underwater robotics. The scope of this paper includes the visual perception\nof underwater robotics with deep learning methods, the available visual\nunderwater datasets, imitation learning, and reinforcement learning methods for\nnavigation. Additionally, relevant works will be categorized under the\nimitation learning or deep learning paradigm for underwater robots for clarity\nof the training methodologies in the current landscape. Literature that uses\ndeep learning algorithms to process non-visual data for underwater navigation\nwill not be considered, except as contrasting examples.",
    "pdf_link": "http://arxiv.org/pdf/2310.19495v1.pdf",
    "authors": [
      "M. Sunbeam"
    ]
  },
  {
    "title": "When deep learning meets security",
    "text": "When deep learning meets security. Deep learning is an emerging research field that has proven its effectiveness\ntowards deploying more efficient intelligent systems. Security, on the other\nhand, is one of the most essential issues in modern communication systems.\nRecently many papers have shown that using deep learning models can achieve\npromising results when applied to the security domain. In this work, we provide\nan overview for the recent studies that apply deep learning techniques to the\nfield of security.",
    "pdf_link": "http://arxiv.org/pdf/1807.04739v1.pdf",
    "authors": [
      "Majd Latah"
    ]
  },
  {
    "title": "Deep Causal Learning for Robotic Intelligence",
    "text": "Deep Causal Learning for Robotic Intelligence. This invited review discusses causal learning in the context of robotic\nintelligence. The paper introduced the psychological findings on causal\nlearning in human cognition, then it introduced the traditional statistical\nsolutions on causal discovery and causal inference. The paper reviewed recent\ndeep causal learning algorithms with a focus on their architectures and the\nbenefits of using deep nets and discussed the gap between deep causal learning\nand the needs of robotic intelligence.",
    "pdf_link": "http://arxiv.org/pdf/2212.12597v1.pdf",
    "authors": [
      "Yangming Li"
    ]
  },
  {
    "title": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art",
    "text": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art. Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.",
    "pdf_link": "http://arxiv.org/pdf/1802.08717v1.pdf",
    "authors": [
      "Maciej A. Mazurowski",
      "Mateusz Buda",
      "Ashirbani Saha",
      "Mustafa R. Bashir"
    ]
  },
  {
    "title": "A Selective Overview of Deep Learning",
    "text": "A Selective Overview of Deep Learning. Deep learning has arguably achieved tremendous success in recent years. In\nsimple words, deep learning uses the composition of many nonlinear functions to\nmodel the complex dependency between input features and labels. While neural\nnetworks have a long history, recent advances have greatly improved their\nperformance in computer vision, natural language processing, etc. From the\nstatistical and scientific perspective, it is natural to ask: What is deep\nlearning? What are the new characteristics of deep learning, compared with\nclassical methods? What are the theoretical foundations of deep learning? To\nanswer these questions, we introduce common neural network models (e.g.,\nconvolutional neural nets, recurrent neural nets, generative adversarial nets)\nand training techniques (e.g., stochastic gradient descent, dropout, batch\nnormalization) from a statistical point of view. Along the way, we highlight\nnew characteristics of deep learning (including depth and over-parametrization)\nand explain their practical and theoretical benefits. We also sample recent\nresults on theories of deep learning, many of which are only suggestive. While\na complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.",
    "pdf_link": "http://arxiv.org/pdf/1904.05526v2.pdf",
    "authors": [
      "Jianqing Fan",
      "Cong Ma",
      "Yiqiao Zhong"
    ]
  },
  {
    "title": "A Survey on Deep Learning Methods for Robot Vision",
    "text": "A Survey on Deep Learning Methods for Robot Vision. Deep learning has allowed a paradigm shift in pattern recognition, from using\nhand-crafted features together with statistical classifiers to using\ngeneral-purpose learning procedures for learning data-driven representations,\nfeatures, and classifiers together. The application of this new paradigm has\nbeen particularly successful in computer vision, in which the development of\ndeep learning methods for vision applications has become a hot research topic.\nGiven that deep learning has already attracted the attention of the robot\nvision community, the main purpose of this survey is to address the use of deep\nlearning in robot vision. To achieve this, a comprehensive overview of deep\nlearning and its usage in computer vision is given, that includes a description\nof the most frequently used neural models and their main application areas.\nThen, the standard methodology and tools used for designing deep-learning based\nvision systems are presented. Afterwards, a review of the principal work using\ndeep learning in robot vision is presented, as well as current and future\ntrends related to the use of deep learning in robotics. This survey is intended\nto be a guide for the developers of robot vision systems.",
    "pdf_link": "http://arxiv.org/pdf/1803.10862v1.pdf",
    "authors": [
      "Javier Ruiz-del-Solar",
      "Patricio Loncomilla",
      "Naiomi Soto"
    ]
  },
  {
    "title": "Interpretations of Deep Learning by Forests and Haar Wavelets",
    "text": "Interpretations of Deep Learning by Forests and Haar Wavelets. This paper presents a basic property of region dividing of ReLU (rectified\nlinear unit) deep learning when new layers are successively added, by which two\nnew perspectives of interpreting deep learning are given. The first is related\nto decision trees and forests; we construct a deep learning structure\nequivalent to a forest in classification abilities, which means that certain\nkinds of ReLU deep learning can be considered as forests. The second\nperspective is that Haar wavelet represented functions can be approximated by\nReLU deep learning with arbitrary precision; and then a general conclusion of\nfunction approximation abilities of ReLU deep learning is given. Finally,\ngeneralize some of the conclusions of ReLU deep learning to the case of\nsigmoid-unit deep learning.",
    "pdf_link": "http://arxiv.org/pdf/1906.06706v7.pdf",
    "authors": [
      "Changcun Huang"
    ]
  },
  {
    "title": "A Brief Survey of Deep Reinforcement Learning",
    "text": "A Brief Survey of Deep Reinforcement Learning. Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.",
    "pdf_link": "http://arxiv.org/pdf/1708.05866v2.pdf",
    "authors": [
      "Kai Arulkumaran",
      "Marc Peter Deisenroth",
      "Miles Brundage",
      "Anil Anthony Bharath"
    ]
  },
  {
    "title": "Topological Deep Learning: A Review of an Emerging Paradigm",
    "text": "Topological Deep Learning: A Review of an Emerging Paradigm. Topological data analysis (TDA) provides insight into data shape. The\nsummaries obtained by these methods are principled global descriptions of\nmulti-dimensional data whilst exhibiting stable properties such as robustness\nto deformation and noise. Such properties are desirable in deep learning\npipelines but they are typically obtained using non-TDA strategies. This is\npartly caused by the difficulty of combining TDA constructs (e.g. barcode and\npersistence diagrams) with current deep learning algorithms. Fortunately, we\nare now witnessing a growth of deep learning applications embracing\ntopologically-guided components. In this survey, we review the nascent field of\ntopological deep learning by first revisiting the core concepts of TDA. We then\nexplore how the use of TDA techniques has evolved over time to support deep\nlearning frameworks, and how they can be integrated into different aspects of\ndeep learning. Furthermore, we touch on TDA usage for analyzing existing deep\nmodels; deep topological analytics. Finally, we discuss the challenges and\nfuture prospects of topological deep learning.",
    "pdf_link": "http://arxiv.org/pdf/2302.03836v1.pdf",
    "authors": [
      "Ali Zia",
      "Abdelwahed Khamis",
      "James Nichols",
      "Zeeshan Hayder",
      "Vivien Rolland",
      "Lars Petersson"
    ]
  },
  {
    "title": "Generalization and Expressivity for Deep Nets",
    "text": "Generalization and Expressivity for Deep Nets. Along with the rapid development of deep learning in practice, the\ntheoretical explanations for its success become urgent. Generalization and\nexpressivity are two widely used measurements to quantify theoretical behaviors\nof deep learning. The expressivity focuses on finding functions expressible by\ndeep nets but cannot be approximated by shallow nets with the similar number of\nneurons. It usually implies the large capacity. The generalization aims at\nderiving fast learning rate for deep nets. It usually requires small capacity\nto reduce the variance. Different from previous studies on deep learning,\npursuing either expressivity or generalization, we take both factors into\naccount to explore the theoretical advantages of deep nets. For this purpose,\nwe construct a deep net with two hidden layers possessing excellent\nexpressivity in terms of localized and sparse approximation. Then, utilizing\nthe well known covering number to measure the capacity, we find that deep nets\npossess excellent expressive power (measured by localized and sparse\napproximation) without enlarging the capacity of shallow nets. As a\nconsequence, we derive near optimal learning rates for implementing empirical\nrisk minimization (ERM) on the constructed deep nets. These results\ntheoretically exhibit the advantage of deep nets from learning theory\nviewpoints.",
    "pdf_link": "http://arxiv.org/pdf/1803.03772v2.pdf",
    "authors": [
      "Shao-Bo Lin"
    ]
  },
  {
    "title": "Deep Incremental Boosting",
    "text": "Deep Incremental Boosting. This paper introduces Deep Incremental Boosting, a new technique derived from\nAdaBoost, specifically adapted to work with Deep Learning methods, that reduces\nthe required training time and improves generalisation. We draw inspiration\nfrom Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.",
    "pdf_link": "http://arxiv.org/pdf/1708.03704v1.pdf",
    "authors": [
      "Alan Mosca",
      "George D Magoulas"
    ]
  },
  {
    "title": "Combining Deep Learning with Good Old-Fashioned Machine Learning",
    "text": "Combining Deep Learning with Good Old-Fashioned Machine Learning. We present a comprehensive, stacking-based framework for combining deep\nlearning with good old-fashioned machine learning, called Deep GOld. Our\nframework involves ensemble selection from 51 retrained pretrained deep\nnetworks as first-level models, and 10 machine-learning algorithms as\nsecond-level models. Enabled by today's state-of-the-art software tools and\nhardware platforms, Deep GOld delivers consistent improvement when tested on\nfour image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny\nImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original\nnetworks' performance.",
    "pdf_link": "http://arxiv.org/pdf/2207.03757v2.pdf",
    "authors": [
      "Moshe Sipper"
    ]
  },
  {
    "title": "Deep frequency principle towards understanding why deeper learning is\n  faster",
    "text": "Deep frequency principle towards understanding why deeper learning is\n  faster. Understanding the effect of depth in deep learning is a critical problem. In\nthis work, we utilize the Fourier analysis to empirically provide a promising\nmechanism to understand why feedforward deeper learning is faster. To this end,\nwe separate a deep neural network, trained by normal stochastic gradient\ndescent, into two parts during analysis, i.e., a pre-condition component and a\nlearning component, in which the output of the pre-condition one is the input\nof the learning one. We use a filtering method to characterize the frequency\ndistribution of a high-dimensional function. Based on experiments of deep\nnetworks and real dataset, we propose a deep frequency principle, that is, the\neffective target function for a deeper hidden layer biases towards lower\nfrequency during the training. Therefore, the learning component effectively\nlearns a lower frequency function if the pre-condition component has more\nlayers. Due to the well-studied frequency principle, i.e., deep neural networks\nlearn lower frequency functions faster, the deep frequency principle provides a\nreasonable explanation to why deeper learning is faster. We believe these\nempirical studies would be valuable for future theoretical studies of the\neffect of depth in deep learning.",
    "pdf_link": "http://arxiv.org/pdf/2007.14313v2.pdf",
    "authors": [
      "Zhi-Qin John Xu",
      "Hanxu Zhou"
    ]
  },
  {
    "title": "Deep Bayesian Active Learning with Image Data",
    "text": "Deep Bayesian Active Learning with Image Data. Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).",
    "pdf_link": "http://arxiv.org/pdf/1703.02910v1.pdf",
    "authors": [
      "Yarin Gal",
      "Riashat Islam",
      "Zoubin Ghahramani"
    ]
  },
  {
    "title": "Deep Learning for Genomics: A Concise Overview",
    "text": "Deep Learning for Genomics: A Concise Overview. Advancements in genomic research such as high-throughput sequencing\ntechniques have driven modern genomic studies into \"big data\" disciplines. This\ndata explosion is constantly challenging conventional methods used in genomics.\nIn parallel with the urgent demand for robust algorithms, deep learning has\nsucceeded in a variety of fields such as vision, speech, and text processing.\nYet genomics entails unique challenges to deep learning since we are expecting\nfrom deep learning a superhuman intelligence that explores beyond our knowledge\nto interpret the genome. A powerful deep learning model should rely on\ninsightful utilization of task-specific knowledge. In this paper, we briefly\ndiscuss the strengths of different deep learning models from a genomic\nperspective so as to fit each particular task with a proper deep architecture,\nand remark on practical considerations of developing modern deep learning\narchitectures for genomics. We also provide a concise review of deep learning\napplications in various aspects of genomic research, as well as pointing out\npotential opportunities and obstacles for future genomics applications.",
    "pdf_link": "http://arxiv.org/pdf/1802.00810v4.pdf",
    "authors": [
      "Tianwei Yue",
      "Yuanxin Wang",
      "Longxiang Zhang",
      "Chunming Gu",
      "Haoru Xue",
      "Wenping Wang",
      "Qi Lyu",
      "Yujie Dun"
    ]
  },
  {
    "title": "Adversarial Attack Based Countermeasures against Deep Learning\n  Side-Channel Attacks",
    "text": "Adversarial Attack Based Countermeasures against Deep Learning\n  Side-Channel Attacks. Numerous previous works have studied deep learning algorithms applied in the\ncontext of side-channel attacks, which demonstrated the ability to perform\nsuccessful key recoveries. These studies show that modern cryptographic devices\nare increasingly threatened by side-channel attacks with the help of deep\nlearning. However, the existing countermeasures are designed to resist\nclassical side-channel attacks, and cannot protect cryptographic devices from\ndeep learning based side-channel attacks. Thus, there arises a strong need for\ncountermeasures against deep learning based side-channel attacks. Although deep\nlearning has the high potential in solving complex problems, it is vulnerable\nto adversarial attacks in the form of subtle perturbations to inputs that lead\na model to predict incorrectly.\n  In this paper, we propose a kind of novel countermeasures based on\nadversarial attacks that is specifically designed against deep learning based\nside-channel attacks. We estimate several models commonly used in deep learning\nbased side-channel attacks to evaluate the proposed countermeasures. It shows\nthat our approach can effectively protect cryptographic devices from deep\nlearning based side-channel attacks in practice. In addition, our experiments\nshow that the new countermeasures can also resist classical side-channel\nattacks.",
    "pdf_link": "http://arxiv.org/pdf/2009.10568v1.pdf",
    "authors": [
      "Ruizhe Gu",
      "Ping Wang",
      "Mengce Zheng",
      "Honggang Hu",
      "Nenghai Yu"
    ]
  },
  {
    "title": "Accelerating Deep Learning with Shrinkage and Recall",
    "text": "Accelerating Deep Learning with Shrinkage and Recall. Deep Learning is a very powerful machine learning model. Deep Learning trains\na large number of parameters for multiple layers and is very slow when data is\nin large scale and the architecture size is large. Inspired from the shrinking\ntechnique used in accelerating computation of Support Vector Machines (SVM)\nalgorithm and screening technique used in LASSO, we propose a shrinking Deep\nLearning with recall (sDLr) approach to speed up deep learning computation. We\nexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\n(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\nsets. Results show that the speedup using shrinking Deep Learning with recall\n(sDLr) can reach more than 2.0 while still giving competitive classification\nperformance.",
    "pdf_link": "http://arxiv.org/pdf/1605.01369v2.pdf",
    "authors": [
      "Shuai Zheng",
      "Abhinav Vishnu",
      "Chris Ding"
    ]
  },
  {
    "title": "Adversarial Robustness of Deep Learning: Theory, Algorithms, and\n  Applications",
    "text": "Adversarial Robustness of Deep Learning: Theory, Algorithms, and\n  Applications. This tutorial aims to introduce the fundamentals of adversarial robustness of\ndeep learning, presenting a well-structured review of up-to-date techniques to\nassess the vulnerability of various types of deep learning models to\nadversarial examples. This tutorial will particularly highlight\nstate-of-the-art techniques in adversarial attacks and robustness verification\nof deep neural networks (DNNs). We will also introduce some effective\ncountermeasures to improve the robustness of deep learning models, with a\nparticular focus on adversarial training. We aim to provide a comprehensive\noverall picture about this emerging direction and enable the community to be\naware of the urgency and importance of designing robust deep learning models in\nsafety-critical data analytical applications, ultimately enabling the end-users\nto trust deep learning classifiers. We will also summarize potential research\ndirections concerning the adversarial robustness of deep learning, and its\npotential benefits to enable accountable and trustworthy deep learning-based\ndata analytical systems and applications.",
    "pdf_link": "http://arxiv.org/pdf/2108.10451v1.pdf",
    "authors": [
      "Wenjie Ruan",
      "Xinping Yi",
      "Xiaowei Huang"
    ]
  },
  {
    "title": "Efficient Deep Feature Learning and Extraction via StochasticNets",
    "text": "Efficient Deep Feature Learning and Extraction via StochasticNets. Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.",
    "pdf_link": "http://arxiv.org/pdf/1512.03844v1.pdf",
    "authors": [
      "Mohammad Javad Shafiee",
      "Parthipan Siva",
      "Paul Fieguth",
      "Alexander Wong"
    ]
  },
  {
    "title": "Modern Deep Reinforcement Learning Algorithms",
    "text": "Modern Deep Reinforcement Learning Algorithms. Recent advances in Reinforcement Learning, grounded on combining classical\ntheoretical results with Deep Learning paradigm, led to breakthroughs in many\nartificial intelligence tasks and gave birth to Deep Reinforcement Learning\n(DRL) as a field of research. In this work latest DRL algorithms are reviewed\nwith a focus on their theoretical justification, practical limitations and\nobserved empirical properties.",
    "pdf_link": "http://arxiv.org/pdf/1906.10025v2.pdf",
    "authors": [
      "Sergey Ivanov",
      "Alexander D'yakonov"
    ]
  },
  {
    "title": "An Essay on Optimization Mystery of Deep Learning",
    "text": "An Essay on Optimization Mystery of Deep Learning. Despite the huge empirical success of deep learning, theoretical\nunderstanding of neural networks learning process is still lacking. This is the\nreason, why some of its features seem \"mysterious\". We emphasize two mysteries\nof deep learning: generalization mystery, and optimization mystery. In this\nessay we review and draw connections between several selected works concerning\nthe latter.",
    "pdf_link": "http://arxiv.org/pdf/1905.07187v1.pdf",
    "authors": [
      "Eugene Golikov"
    ]
  },
  {
    "title": "Deep Super Learner: A Deep Ensemble for Classification Problems",
    "text": "Deep Super Learner: A Deep Ensemble for Classification Problems. Deep learning has become very popular for tasks such as predictive modeling\nand pattern recognition in handling big data. Deep learning is a powerful\nmachine learning method that extracts lower level features and feeds them\nforward for the next layer to identify higher level features that improve\nperformance. However, deep neural networks have drawbacks, which include many\nhyper-parameters and infinite architectures, opaqueness into results, and\nrelatively slower convergence on smaller datasets. While traditional machine\nlearning algorithms can address these drawbacks, they are not typically capable\nof the performance levels achieved by deep neural networks. To improve\nperformance, ensemble methods are used to combine multiple base learners. Super\nlearning is an ensemble that finds the optimal combination of diverse learning\nalgorithms. This paper proposes deep super learning as an approach which\nachieves log loss and accuracy results competitive to deep neural networks\nwhile employing traditional machine learning algorithms in a hierarchical\nstructure. The deep super learner is flexible, adaptable, and easy to train\nwith good performance across different tasks using identical hyper-parameter\nvalues. Using traditional machine learning requires fewer hyper-parameters,\nallows transparency into results, and has relatively fast convergence on\nsmaller datasets. Experimental results show that the deep super learner has\nsuperior performance compared to the individual base learners, single-layer\nensembles, and in some cases deep neural networks. Performance of the deep\nsuper learner may further be improved with task-specific tuning.",
    "pdf_link": "http://arxiv.org/pdf/1803.02323v1.pdf",
    "authors": [
      "Steven Young",
      "Tamer Abdou",
      "Ayse Bener"
    ]
  },
  {
    "title": "Deep Embedding Kernel",
    "text": "Deep Embedding Kernel. In this paper, we propose a novel supervised learning method that is called\nDeep Embedding Kernel (DEK). DEK combines the advantages of deep learning and\nkernel methods in a unified framework. More specifically, DEK is a learnable\nkernel represented by a newly designed deep architecture. Compared with\npre-defined kernels, this kernel can be explicitly trained to map data to an\noptimized high-level feature space where data may have favorable features\ntoward the application. Compared with typical deep learning using SoftMax or\nlogistic regression as the top layer, DEK is expected to be more generalizable\nto new data. Experimental results show that DEK has superior performance than\ntypical machine learning methods in identity detection, classification,\nregression, dimension reduction, and transfer learning.",
    "pdf_link": "http://arxiv.org/pdf/1804.05806v1.pdf",
    "authors": [
      "Linh Le",
      "Ying Xie"
    ]
  },
  {
    "title": "Priors in Bayesian Deep Learning: A Review",
    "text": "Priors in Bayesian Deep Learning: A Review. While the choice of prior is one of the most critical parts of the Bayesian\ninference workflow, recent Bayesian deep learning models have often fallen back\non vague priors, such as standard Gaussians. In this review, we highlight the\nimportance of prior choices for Bayesian deep learning and present an overview\nof different priors that have been proposed for (deep) Gaussian processes,\nvariational autoencoders, and Bayesian neural networks. We also outline\ndifferent methods of learning priors for these models from data. We hope to\nmotivate practitioners in Bayesian deep learning to think more carefully about\nthe prior specification for their models and to provide them with some\ninspiration in this regard.",
    "pdf_link": "http://arxiv.org/pdf/2105.06868v3.pdf",
    "authors": [
      "Vincent Fortuin"
    ]
  },
  {
    "title": "Lecture Notes: Optimization for Machine Learning",
    "text": "Lecture Notes: Optimization for Machine Learning. Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.",
    "pdf_link": "http://arxiv.org/pdf/1909.03550v1.pdf",
    "authors": [
      "Elad Hazan"
    ]
  },
  {
    "title": "An Optimal Control View of Adversarial Machine Learning",
    "text": "An Optimal Control View of Adversarial Machine Learning. I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.",
    "pdf_link": "http://arxiv.org/pdf/1811.04422v1.pdf",
    "authors": [
      "Xiaojin Zhu"
    ]
  },
  {
    "title": "Minimax deviation strategies for machine learning and recognition with\n  short learning samples",
    "text": "Minimax deviation strategies for machine learning and recognition with\n  short learning samples. The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.",
    "pdf_link": "http://arxiv.org/pdf/1707.04849v1.pdf",
    "authors": [
      "Michail Schlesinger",
      "Evgeniy Vodolazskiy"
    ]
  },
  {
    "title": "Machine Learning for Clinical Predictive Analytics",
    "text": "Machine Learning for Clinical Predictive Analytics. In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.",
    "pdf_link": "http://arxiv.org/pdf/1909.09246v1.pdf",
    "authors": [
      "Wei-Hung Weng"
    ]
  },
  {
    "title": "Towards Modular Machine Learning Solution Development: Benefits and\n  Trade-offs",
    "text": "Towards Modular Machine Learning Solution Development: Benefits and\n  Trade-offs. Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.",
    "pdf_link": "http://arxiv.org/pdf/2301.09753v1.pdf",
    "authors": [
      "Samiyuru Menik",
      "Lakshmish Ramaswamy"
    ]
  },
  {
    "title": "Introduction to Machine Learning: Class Notes 67577",
    "text": "Introduction to Machine Learning: Class Notes 67577. Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
    "pdf_link": "http://arxiv.org/pdf/0904.3664v1.pdf",
    "authors": [
      "Amnon Shashua"
    ]
  },
  {
    "title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
    "text": "The Tribes of Machine Learning and the Realm of Computer Architecture. Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.",
    "pdf_link": "http://arxiv.org/pdf/2012.04105v1.pdf",
    "authors": [
      "Ayaz Akram",
      "Jason Lowe-Power"
    ]
  },
  {
    "title": "A Machine Learning Tutorial for Operational Meteorology, Part I:\n  Traditional Machine Learning",
    "text": "A Machine Learning Tutorial for Operational Meteorology, Part I:\n  Traditional Machine Learning. Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.",
    "pdf_link": "http://arxiv.org/pdf/2204.07492v2.pdf",
    "authors": [
      "Randy J. Chase",
      "David R. Harrison",
      "Amanda Burke",
      "Gary M. Lackmann",
      "Amy McGovern"
    ]
  },
  {
    "title": "Position Paper: Towards Transparent Machine Learning",
    "text": "Position Paper: Towards Transparent Machine Learning. Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.",
    "pdf_link": "http://arxiv.org/pdf/1911.06612v1.pdf",
    "authors": [
      "Dustin Juliano"
    ]
  },
  {
    "title": "Understanding Bias in Machine Learning",
    "text": "Understanding Bias in Machine Learning. Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.",
    "pdf_link": "http://arxiv.org/pdf/1909.01866v1.pdf",
    "authors": [
      "Jindong Gu",
      "Daniela Oelke"
    ]
  },
  {
    "title": "A Unified Analytical Framework for Trustable Machine Learning and\n  Automation Running with Blockchain",
    "text": "A Unified Analytical Framework for Trustable Machine Learning and\n  Automation Running with Blockchain. Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.",
    "pdf_link": "http://arxiv.org/pdf/1903.08801v1.pdf",
    "authors": [
      "Tao Wang"
    ]
  },
  {
    "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?",
    "text": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?. We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.",
    "pdf_link": "http://arxiv.org/pdf/1707.09562v3.pdf",
    "authors": [
      "Yu Liu",
      "Hantian Zhang",
      "Luyuan Zeng",
      "Wentao Wu",
      "Ce Zhang"
    ]
  },
  {
    "title": "Data Pricing in Machine Learning Pipelines",
    "text": "Data Pricing in Machine Learning Pipelines. Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.",
    "pdf_link": "http://arxiv.org/pdf/2108.07915v1.pdf",
    "authors": [
      "Zicun Cong",
      "Xuan Luo",
      "Pei Jian",
      "Feida Zhu",
      "Yong Zhang"
    ]
  },
  {
    "title": "Techniques for Automated Machine Learning",
    "text": "Techniques for Automated Machine Learning. Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.",
    "pdf_link": "http://arxiv.org/pdf/1907.08908v1.pdf",
    "authors": [
      "Yi-Wei Chen",
      "Qingquan Song",
      "Xia Hu"
    ]
  },
  {
    "title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
    "text": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning. With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.",
    "pdf_link": "http://arxiv.org/pdf/2312.03120v1.pdf",
    "authors": [
      "Omer Subasi",
      "Oceane Bel",
      "Joseph Manzano",
      "Kevin Barker"
    ]
  },
  {
    "title": "Parallelization of Machine Learning Algorithms Respectively on Single\n  Machine and Spark",
    "text": "Parallelization of Machine Learning Algorithms Respectively on Single\n  Machine and Spark. With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.",
    "pdf_link": "http://arxiv.org/pdf/2206.07090v2.pdf",
    "authors": [
      "Jiajun Shen"
    ]
  },
  {
    "title": "AutoCompete: A Framework for Machine Learning Competition",
    "text": "AutoCompete: A Framework for Machine Learning Competition. In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.",
    "pdf_link": "http://arxiv.org/pdf/1507.02188v1.pdf",
    "authors": [
      "Abhishek Thakur",
      "Artus Krohn-Grimberghe"
    ]
  },
  {
    "title": "Joint Training of Deep Boltzmann Machines",
    "text": "Joint Training of Deep Boltzmann Machines. We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.",
    "pdf_link": "http://arxiv.org/pdf/1212.2686v1.pdf",
    "authors": [
      "Ian Goodfellow",
      "Aaron Courville",
      "Yoshua Bengio"
    ]
  },
  {
    "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications",
    "text": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications. This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.",
    "pdf_link": "http://arxiv.org/pdf/1607.02450v2.pdf",
    "authors": [
      "Kush R. Varshney"
    ]
  },
  {
    "title": "Mathematical Perspective of Machine Learning",
    "text": "Mathematical Perspective of Machine Learning. We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.",
    "pdf_link": "http://arxiv.org/pdf/2007.01503v1.pdf",
    "authors": [
      "Yarema Boryshchak"
    ]
  },
  {
    "title": "Private Machine Learning via Randomised Response",
    "text": "Private Machine Learning via Randomised Response. We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.",
    "pdf_link": "http://arxiv.org/pdf/2001.04942v2.pdf",
    "authors": [
      "David Barber"
    ]
  },
  {
    "title": "Ten-year Survival Prediction for Breast Cancer Patients",
    "text": "Ten-year Survival Prediction for Breast Cancer Patients. This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.",
    "pdf_link": "http://arxiv.org/pdf/1911.00776v1.pdf",
    "authors": [
      "Changmao Li",
      "Han He",
      "Yunze Hao",
      "Caleb Ziems"
    ]
  },
  {
    "title": "A Survey of Optimization Methods from a Machine Learning Perspective",
    "text": "A Survey of Optimization Methods from a Machine Learning Perspective. Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.",
    "pdf_link": "http://arxiv.org/pdf/1906.06821v2.pdf",
    "authors": [
      "Shiliang Sun",
      "Zehui Cao",
      "Han Zhu",
      "Jing Zhao"
    ]
  },
  {
    "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
    "text": "When Machine Learning Meets Privacy: A Survey and Outlook. The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.",
    "pdf_link": "http://arxiv.org/pdf/2011.11819v1.pdf",
    "authors": [
      "Bo Liu",
      "Ming Ding",
      "Sina Shaham",
      "Wenny Rahayu",
      "Farhad Farokhi",
      "Zihuai Lin"
    ]
  },
  {
    "title": "Augmented Q Imitation Learning (AQIL)",
    "text": "Augmented Q Imitation Learning (AQIL). The study of unsupervised learning can be generally divided into two\ncategories: imitation learning and reinforcement learning. In imitation\nlearning the machine learns by mimicking the behavior of an expert system\nwhereas in reinforcement learning the machine learns via direct environment\nfeedback. Traditional deep reinforcement learning takes a significant time\nbefore the machine starts to converge to an optimal policy. This paper proposes\nAugmented Q-Imitation-Learning, a method by which deep reinforcement learning\nconvergence can be accelerated by applying Q-imitation-learning as the initial\ntraining process in traditional Deep Q-learning.",
    "pdf_link": "http://arxiv.org/pdf/2004.00993v2.pdf",
    "authors": [
      "Xiao Lei Zhang",
      "Anish Agarwal"
    ]
  },
  {
    "title": "Probabilistic Machine Learning for Healthcare",
    "text": "Probabilistic Machine Learning for Healthcare. Machine learning can be used to make sense of healthcare data. Probabilistic\nmachine learning models help provide a complete picture of observed data in\nhealthcare. In this review, we examine how probabilistic machine learning can\nadvance healthcare. We consider challenges in the predictive model building\npipeline where probabilistic models can be beneficial including calibration and\nmissing data. Beyond predictive models, we also investigate the utility of\nprobabilistic machine learning models in phenotyping, in generative models for\nclinical use cases, and in reinforcement learning.",
    "pdf_link": "http://arxiv.org/pdf/2009.11087v1.pdf",
    "authors": [
      "Irene Y. Chen",
      "Shalmali Joshi",
      "Marzyeh Ghassemi",
      "Rajesh Ranganath"
    ]
  },
  {
    "title": "Evaluation Challenges for Geospatial ML",
    "text": "Evaluation Challenges for Geospatial ML. As geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is\nimperative to evaluate their accuracy and applicability. Geospatial machine\nlearning has key distinctions from other learning paradigms, and as such, the\ncorrect way to measure performance of spatial machine learning outputs has been\na topic of debate. In this paper, I delineate unique challenges of model\nevaluation for geospatial machine learning with global or remotely sensed\ndatasets, culminating in concrete takeaways to improve evaluations of\ngeospatial model performance.",
    "pdf_link": "http://arxiv.org/pdf/2303.18087v1.pdf",
    "authors": [
      "Esther Rolf"
    ]
  },
  {
    "title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault\n  Tolerance",
    "text": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault\n  Tolerance. Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and\nbusiness circles. In this paper, we offer a comprehensive and unbiased review\nof the various concepts that have emerged in the field of quantum machine\nlearning. This includes techniques used in Noisy Intermediate-Scale Quantum\n(NISQ) technologies and approaches for algorithms compatible with\nfault-tolerant quantum computing hardware. Our review covers fundamental\nconcepts, algorithms, and the statistical learning theory pertinent to quantum\nmachine learning.",
    "pdf_link": "http://arxiv.org/pdf/2401.11351v2.pdf",
    "authors": [
      "Yunfei Wang",
      "Junyu Liu"
    ]
  },
  {
    "title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality\n  Assurance Methodology",
    "text": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality\n  Assurance Methodology. Machine learning is an established and frequently used technique in industry\nand academia but a standard process model to improve success and efficiency of\nmachine learning applications is still missing. Project organizations and\nmachine learning practitioners have a need for guidance throughout the life\ncycle of a machine learning application to meet business expectations. We\ntherefore propose a process model for the development of machine learning\napplications, that covers six phases from defining the scope to maintaining the\ndeployed machine learning application. The first phase combines business and\ndata understanding as data availability oftentimes affects the feasibility of\nthe project. The sixth phase covers state-of-the-art approaches for monitoring\nand maintenance of a machine learning applications, as the risk of model\ndegradation in a changing environment is eminent. With each task of the\nprocess, we propose quality assurance methodology that is suitable to adress\nchallenges in machine learning development that we identify in form of risks.\nThe methodology is drawn from practical experience and scientific literature\nand has proven to be general and stable. The process model expands on CRISP-DM,\na data mining process model that enjoys strong industry support but lacks to\naddress machine learning specific tasks. Our work proposes an industry and\napplication neutral process model tailored for machine learning applications\nwith focus on technical tasks for quality assurance.",
    "pdf_link": "http://arxiv.org/pdf/2003.05155v2.pdf",
    "authors": [
      "Stefan Studer",
      "Thanh Binh Bui",
      "Christian Drescher",
      "Alexander Hanuschkin",
      "Ludwig Winkler",
      "Steven Peters",
      "Klaus-Robert Mueller"
    ]
  },
  {
    "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of\n  learning relational order via reinforcement learning procedure?",
    "text": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of\n  learning relational order via reinforcement learning procedure?. In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.",
    "pdf_link": "http://arxiv.org/pdf/1706.08001v1.pdf",
    "authors": [
      "Zizhuang Wang"
    ]
  },
  {
    "title": "Machine Learning Potential Repository",
    "text": "Machine Learning Potential Repository. This paper introduces a machine learning potential repository that includes\nPareto optimal machine learning potentials. It also shows the systematic\ndevelopment of accurate and fast machine learning potentials for a wide range\nof elemental systems. As a result, many Pareto optimal machine learning\npotentials are available in the repository from a website. Therefore, the\nrepository will help many scientists to perform accurate and fast atomistic\nsimulations.",
    "pdf_link": "http://arxiv.org/pdf/2007.14206v1.pdf",
    "authors": [
      "Atsuto Seko"
    ]
  },
  {
    "title": "Quantum memristors for neuromorphic quantum machine learning",
    "text": "Quantum memristors for neuromorphic quantum machine learning. Quantum machine learning may permit to realize more efficient machine\nlearning calculations with near-term quantum devices. Among the diverse quantum\nmachine learning paradigms which are currently being considered, quantum\nmemristors are promising as a way of combining, in the same quantum hardware, a\nunitary evolution with the nonlinearity provided by the measurement and\nfeedforward. Thus, an efficient way of deploying neuromorphic quantum computing\nfor quantum machine learning may be enabled.",
    "pdf_link": "http://arxiv.org/pdf/2412.18979v1.pdf",
    "authors": [
      "Lucas Lamata"
    ]
  },
  {
    "title": "metric-learn: Metric Learning Algorithms in Python",
    "text": "metric-learn: Metric Learning Algorithms in Python. metric-learn is an open source Python package implementing supervised and\nweakly-supervised distance metric learning algorithms. As part of\nscikit-learn-contrib, it provides a unified interface compatible with\nscikit-learn which allows to easily perform cross-validation, model selection,\nand pipelining with other machine learning estimators. metric-learn is\nthoroughly tested and available on PyPi under the MIT licence.",
    "pdf_link": "http://arxiv.org/pdf/1908.04710v3.pdf",
    "authors": [
      "William de Vazelhes",
      "CJ Carey",
      "Yuan Tang",
      "Nathalie Vauquier",
      "Aur\u00e9lien Bellet"
    ]
  },
  {
    "title": "Theoretical Models of Learning to Learn",
    "text": "Theoretical Models of Learning to Learn. A Machine can only learn if it is biased in some way. Typically the bias is\nsupplied by hand, for example through the choice of an appropriate set of\nfeatures. However, if the learning machine is embedded within an {\\em\nenvironment} of related tasks, then it can {\\em learn} its own bias by learning\nsufficiently many tasks from the environment. In this paper two models of bias\nlearning (or equivalently, learning to learn) are introduced and the main\ntheoretical results presented. The first model is a PAC-type model based on\nempirical process theory, while the second is a hierarchical Bayes model.",
    "pdf_link": "http://arxiv.org/pdf/2002.12364v1.pdf",
    "authors": [
      "Jonathan Baxter"
    ]
  },
  {
    "title": "On-the-Fly Learning in a Perpetual Learning Machine",
    "text": "On-the-Fly Learning in a Perpetual Learning Machine. Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.",
    "pdf_link": "http://arxiv.org/pdf/1509.00913v3.pdf",
    "authors": [
      "Andrew J. R. Simpson"
    ]
  },
  {
    "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n  in Machine Learning",
    "text": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n  in Machine Learning. We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.",
    "pdf_link": "http://arxiv.org/pdf/1607.01400v1.pdf",
    "authors": [
      "Young Woong Park",
      "Diego Klabjan"
    ]
  },
  {
    "title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective",
    "text": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective. Though technical advance of artificial intelligence and machine learning has\nenabled many promising intelligent systems, many computing tasks are still not\nable to be fully accomplished by machine intelligence. Motivated by the\ncomplementary nature of human and machine intelligence, an emerging trend is to\ninvolve humans in the loop of machine learning and decision-making. In this\npaper, we provide a macro-micro review of human-in-the-loop machine learning.\nWe first describe major machine learning challenges which can be addressed by\nhuman intervention in the loop. Then we examine closely the latest research and\nfindings of introducing humans into each step of the lifecycle of machine\nlearning. Finally, we analyze current research gaps and point out future\nresearch directions.",
    "pdf_link": "http://arxiv.org/pdf/2202.10564v1.pdf",
    "authors": [
      "Jiangtao Wang",
      "Bin Guo",
      "Liming Chen"
    ]
  },
  {
    "title": "Can Machines Learn the True Probabilities?",
    "text": "Can Machines Learn the True Probabilities?. When there exists uncertainty, AI machines are designed to make decisions so\nas to reach the best expected outcomes. Expectations are based on true facts\nabout the objective environment the machines interact with, and those facts can\nbe encoded into AI models in the form of true objective probability functions.\nAccordingly, AI models involve probabilistic machine learning in which the\nprobabilities should be objectively interpreted. We prove under some basic\nassumptions when machines can learn the true objective probabilities, if any,\nand when machines cannot learn them.",
    "pdf_link": "http://arxiv.org/pdf/2407.05526v1.pdf",
    "authors": [
      "Jinsook Kim"
    ]
  },
  {
    "title": "Scientific Machine Learning Benchmarks",
    "text": "Scientific Machine Learning Benchmarks. The breakthrough in Deep Learning neural networks has transformed the use of\nAI and machine learning technologies for the analysis of very large\nexperimental datasets. These datasets are typically generated by large-scale\nexperimental facilities at national laboratories. In the context of science,\nscientific machine learning focuses on training machines to identify patterns,\ntrends, and anomalies to extract meaningful scientific insights from such\ndatasets. With a new generation of experimental facilities, the rate of data\ngeneration and the scale of data volumes will increasingly require the use of\nmore automated data analysis. At present, identifying the most appropriate\nmachine learning algorithm for the analysis of any given scientific dataset is\nstill a challenge for scientists. This is due to many different machine\nlearning frameworks, computer architectures, and machine learning models.\nHistorically, for modelling and simulation on HPC systems such problems have\nbeen addressed through benchmarking computer applications, algorithms, and\narchitectures. Extending such a benchmarking approach and identifying metrics\nfor the application of machine learning methods to scientific datasets is a new\nchallenge for both scientists and computer scientists. In this paper, we\ndescribe our approach to the development of scientific machine learning\nbenchmarks and review other approaches to benchmarking scientific machine\nlearning.",
    "pdf_link": "http://arxiv.org/pdf/2110.12773v1.pdf",
    "authors": [
      "Jeyan Thiyagalingam",
      "Mallikarjun Shankar",
      "Geoffrey Fox",
      "Tony Hey"
    ]
  },
  {
    "title": "Some Insights into Lifelong Reinforcement Learning Systems",
    "text": "Some Insights into Lifelong Reinforcement Learning Systems. A lifelong reinforcement learning system is a learning system that has the\nability to learn through trail-and-error interaction with the environment over\nits lifetime. In this paper, I give some arguments to show that the traditional\nreinforcement learning paradigm fails to model this type of learning system.\nSome insights into lifelong reinforcement learning are provided, along with a\nsimplistic prototype lifelong reinforcement learning system.",
    "pdf_link": "http://arxiv.org/pdf/2001.09608v1.pdf",
    "authors": [
      "Changjian Li"
    ]
  },
  {
    "title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "text": "Towards A Rigorous Science of Interpretable Machine Learning. As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.",
    "pdf_link": "http://arxiv.org/pdf/1702.08608v2.pdf",
    "authors": [
      "Finale Doshi-Velez",
      "Been Kim"
    ]
  },
  {
    "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project",
    "text": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project. Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.",
    "pdf_link": "http://arxiv.org/pdf/1705.07538v2.pdf",
    "authors": [
      "Peter Bailis",
      "Kunle Olukotun",
      "Christopher Re",
      "Matei Zaharia"
    ]
  },
  {
    "title": "Solving machine learning optimization problems using quantum computers",
    "text": "Solving machine learning optimization problems using quantum computers. Classical optimization algorithms in machine learning often take a long time\nto compute when applied to a multi-dimensional problem and require a huge\namount of CPU and GPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical model to\nleverage quantum parallelism to speed-up machine learning algorithms. We also\napply quantum machine learning and quantum parallelism applied to a\n$3$-dimensional image that vary with time.",
    "pdf_link": "http://arxiv.org/pdf/1911.08587v1.pdf",
    "authors": [
      "Venkat R. Dasari",
      "Mee Seong Im",
      "Lubjana Beshaj"
    ]
  },
  {
    "title": "Bayesian Optimization for Machine Learning : A Practical Guidebook",
    "text": "Bayesian Optimization for Machine Learning : A Practical Guidebook. The engineering of machine learning systems is still a nascent field; relying\non a seemingly daunting collection of quickly evolving tools and best\npractices. It is our hope that this guidebook will serve as a useful resource\nfor machine learning practitioners looking to take advantage of Bayesian\noptimization techniques. We outline four example machine learning problems that\ncan be solved using open source machine learning libraries, and highlight the\nbenefits of using Bayesian optimization in the context of these common machine\nlearning applications.",
    "pdf_link": "http://arxiv.org/pdf/1612.04858v1.pdf",
    "authors": [
      "Ian Dewancker",
      "Michael McCourt",
      "Scott Clark"
    ]
  },
  {
    "title": "Techniques for Interpretable Machine Learning",
    "text": "Techniques for Interpretable Machine Learning. Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.",
    "pdf_link": "http://arxiv.org/pdf/1808.00033v3.pdf",
    "authors": [
      "Mengnan Du",
      "Ninghao Liu",
      "Xia Hu"
    ]
  },
  {
    "title": "Lale: Consistent Automated Machine Learning",
    "text": "Lale: Consistent Automated Machine Learning. Automated machine learning makes it easier for data scientists to develop\npipelines by searching over possible choices for hyperparameters, algorithms,\nand even pipeline topologies. Unfortunately, the syntax for automated machine\nlearning tools is inconsistent with manual machine learning, with each other,\nand with error checks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces Lale, a\nlibrary of high-level Python interfaces that simplifies and unifies automated\nmachine learning in a consistent way.",
    "pdf_link": "http://arxiv.org/pdf/2007.01977v1.pdf",
    "authors": [
      "Guillaume Baudart",
      "Martin Hirzel",
      "Kiran Kate",
      "Parikshit Ram",
      "Avraham Shinnar"
    ]
  },
  {
    "title": "Differential Replication in Machine Learning",
    "text": "Differential Replication in Machine Learning. When deployed in the wild, machine learning models are usually confronted\nwith data and requirements that constantly vary, either because of changes in\nthe generating distribution or because external constraints change the\nenvironment where the model operates. To survive in such an ecosystem, machine\nlearning models need to adapt to new conditions by evolving over time. The idea\nof model adaptability has been studied from different perspectives. In this\npaper, we propose a solution based on reusing the knowledge acquired by the\nalready deployed machine learning models and leveraging it to train future\ngenerations. This is the idea behind differential replication of machine\nlearning models.",
    "pdf_link": "http://arxiv.org/pdf/2007.07981v1.pdf",
    "authors": [
      "Irene Unceta",
      "Jordi Nin",
      "Oriol Pujol"
    ]
  },
  {
    "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis",
    "text": "mlr3proba: An R Package for Machine Learning in Survival Analysis. As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.",
    "pdf_link": "http://arxiv.org/pdf/2008.08080v2.pdf",
    "authors": [
      "Raphael Sonabend",
      "Franz J. Kir\u00e1ly",
      "Andreas Bender",
      "Bernd Bischl",
      "Michel Lang"
    ]
  },
  {
    "title": "Teaching Uncertainty Quantification in Machine Learning through Use\n  Cases",
    "text": "Teaching Uncertainty Quantification in Machine Learning through Use\n  Cases. Uncertainty in machine learning is not generally taught as general knowledge\nin Machine Learning course curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in machine learning, and complement\nthe course with a selection of use cases, aimed to trigger discussion and let\nstudents play with the concepts of uncertainty in a programming setting. Our\nuse cases cover the concept of output uncertainty, Bayesian neural networks and\nweight distributions, sources of uncertainty, and out of distribution\ndetection. We expect that this curriculum and set of use cases motivates the\ncommunity to adopt these important concepts into courses for safety in AI.",
    "pdf_link": "http://arxiv.org/pdf/2108.08712v1.pdf",
    "authors": [
      "Matias Valdenegro-Toro"
    ]
  },
  {
    "title": "Introduction to Machine Learning for Physicians: A Survival Guide for\n  Data Deluge",
    "text": "Introduction to Machine Learning for Physicians: A Survival Guide for\n  Data Deluge. Many modern research fields increasingly rely on collecting and analysing\nmassive, often unstructured, and unwieldy datasets. Consequently, there is\ngrowing interest in machine learning and artificial intelligence applications\nthat can harness this `data deluge'. This broad nontechnical overview provides\na gentle introduction to machine learning with a specific focus on medical and\nbiological applications. We explain the common types of machine learning\nalgorithms and typical tasks that can be solved, illustrating the basics with\nconcrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered\nmedicine.",
    "pdf_link": "http://arxiv.org/pdf/2212.12303v1.pdf",
    "authors": [
      "Ri\u010dards Marcinkevi\u010ds",
      "Ece Ozkan",
      "Julia E. Vogt"
    ]
  },
  {
    "title": "Machine learning-assisted close-set X-ray diffraction phase\n  identification of transition metals",
    "text": "Machine learning-assisted close-set X-ray diffraction phase\n  identification of transition metals. Machine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for\nusing machine learning to predict crystal structure phases from X-ray\ndiffraction data of transition metals and their oxides. We evaluate the\nperformance of our method and compare the variety of its settings. Our results\ndemonstrate that the proposed machine learning framework achieves competitive\nperformance. This demonstrates the potential for machine learning to\nsignificantly impact the field of X-ray diffraction and crystal structure\ndetermination. Open-source implementation:\nhttps://github.com/maxnygma/NeuralXRD.",
    "pdf_link": "http://arxiv.org/pdf/2305.15410v1.pdf",
    "authors": [
      "Maksim Zhdanov",
      "Andrey Zhdanov"
    ]
  },
  {
    "title": "Insights From Insurance for Fair Machine Learning",
    "text": "Insights From Insurance for Fair Machine Learning. We argue that insurance can act as an analogon for the social situatedness of\nmachine learning systems, hence allowing machine learning scholars to take\ninsights from the rich and interdisciplinary insurance literature. Tracing the\ninteraction of uncertainty, fairness and responsibility in insurance provides a\nfresh perspective on fairness in machine learning. We link insurance fairness\nconceptions to their machine learning relatives, and use this bridge to\nproblematize fairness as calibration. In this process, we bring to the\nforefront two themes that have been largely overlooked in the machine learning\nliterature: responsibility and aggregate-individual tensions.",
    "pdf_link": "http://arxiv.org/pdf/2306.14624v2.pdf",
    "authors": [
      "Christian Fr\u00f6hlich",
      "Robert C. Williamson"
    ]
  },
  {
    "title": "Quantum Dynamics of Machine Learning",
    "text": "Quantum Dynamics of Machine Learning. The quantum dynamic equation (QDE) of machine learning is obtained based on\nSchr\\\"odinger equation and potential energy equivalence relationship. Through\nWick rotation, the relationship between quantum dynamics and thermodynamics is\nalso established in this paper. This equation reformulates the iterative\nprocess of machine learning into a time-dependent partial differential equation\nwith a clear mathematical structure, offering a theoretical framework for\ninvestigating machine learning iterations through quantum and mathematical\ntheories. Within this framework, the fundamental iterative process, the\ndiffusion model, and the Softmax and Sigmoid functions are examined, validating\nthe proposed quantum dynamics equations. This approach not only presents a\nrigorous theoretical foundation for machine learning but also holds promise for\nsupporting the implementation of machine learning algorithms on quantum\ncomputers.",
    "pdf_link": "http://arxiv.org/pdf/2407.19890v1.pdf",
    "authors": [
      "Peng Wang",
      "Maimaitiniyazi Maimaitiabudula"
    ]
  },
  {
    "title": "On the Conditions for Domain Stability for Machine Learning: a\n  Mathematical Approach",
    "text": "On the Conditions for Domain Stability for Machine Learning: a\n  Mathematical Approach. This work proposes a mathematical approach that (re)defines a property of\nMachine Learning models named stability and determines sufficient conditions to\nvalidate it. Machine Learning models are represented as functions, and the\ncharacteristics in scope depend upon the domain of the function, what allows us\nto adopt topological and metric spaces theory as a basis. Finally, this work\nprovides some equivalences useful to prove and test stability in Machine\nLearning models. The results suggest that whenever stability is aligned with\nthe notion of function smoothness, then the stability of Machine Learning\nmodels primarily depends upon certain topological, measurable properties of the\nclassification sets within the ML model domain.",
    "pdf_link": "http://arxiv.org/pdf/2412.00464v1.pdf",
    "authors": [
      "Gabriel Pedroza"
    ]
  },
  {
    "title": "Distributed Multitask Learning",
    "text": "Distributed Multitask Learning. We consider the problem of distributed multi-task learning, where each\nmachine learns a separate, but related, task. Specifically, each machine learns\na linear predictor in high-dimensional space,where all tasks share the same\nsmall support. We present a communication-efficient estimator based on the\ndebiased lasso and show that it is comparable with the optimal centralized\nmethod.",
    "pdf_link": "http://arxiv.org/pdf/1510.00633v1.pdf",
    "authors": [
      "Jialei Wang",
      "Mladen Kolar",
      "Nathan Srebro"
    ]
  },
  {
    "title": "Lecture Notes: Neural Network Architectures",
    "text": "Lecture Notes: Neural Network Architectures. These lecture notes provide an overview of Neural Network architectures from\na mathematical point of view. Especially, Machine Learning with Neural Networks\nis seen as an optimization problem. Covered are an introduction to Neural\nNetworks and the following architectures: Feedforward Neural Network,\nConvolutional Neural Network, ResNet, and Recurrent Neural Network.",
    "pdf_link": "http://arxiv.org/pdf/2304.05133v2.pdf",
    "authors": [
      "Evelyn Herberg"
    ]
  },
  {
    "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
    "text": "Self-Organizing Multilayered Neural Networks of Optimal Complexity. The principles of self-organizing the neural networks of optimal complexity\nis considered under the unrepresentative learning set. The method of\nself-organizing the multi-layered neural networks is offered and used to train\nthe logical neural networks which were applied to the medical diagnostics.",
    "pdf_link": "http://arxiv.org/pdf/cs/0504056v1.pdf",
    "authors": [
      "V. Schetinin"
    ]
  },
  {
    "title": "Neural Network Processing Neural Networks: An efficient way to learn\n  higher order functions",
    "text": "Neural Network Processing Neural Networks: An efficient way to learn\n  higher order functions. Functions are rich in meaning and can be interpreted in a variety of ways.\nNeural networks were proven to be capable of approximating a large class of\nfunctions[1]. In this paper, we propose a new class of neural networks called\n\"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural\nnetworks and numerical values, instead of just numerical values. Thus enabling\nneural networks to represent and process rich structures.",
    "pdf_link": "http://arxiv.org/pdf/1911.05640v2.pdf",
    "authors": [
      "Firat Tuna"
    ]
  },
  {
    "title": "Guaranteed Quantization Error Computation for Neural Network Model\n  Compression",
    "text": "Guaranteed Quantization Error Computation for Neural Network Model\n  Compression. Neural network model compression techniques can address the computation issue\nof deep neural networks on embedded devices in industrial systems. The\nguaranteed output error computation problem for neural network compression with\nquantization is addressed in this paper. A merged neural network is built from\na feedforward neural network and its quantized version to produce the exact\noutput difference between two neural networks. Then, optimization-based methods\nand reachability analysis methods are applied to the merged neural network to\ncompute the guaranteed quantization error. Finally, a numerical example is\nproposed to validate the applicability and effectiveness of the proposed\napproach.",
    "pdf_link": "http://arxiv.org/pdf/2304.13812v1.pdf",
    "authors": [
      "Wesley Cooke",
      "Zihao Mo",
      "Weiming Xiang"
    ]
  },
  {
    "title": "Graph Structure of Neural Networks",
    "text": "Graph Structure of Neural Networks. Neural networks are often represented as graphs of connections between\nneurons. However, despite their wide use, there is currently little\nunderstanding of the relationship between the graph structure of the neural\nnetwork and its predictive performance. Here we systematically investigate how\ndoes the graph structure of neural networks affect their predictive\nperformance. To this end, we develop a novel graph-based representation of\nneural networks called relational graph, where layers of neural network\ncomputation correspond to rounds of message exchange along the graph structure.\nUsing this representation we show that: (1) a \"sweet spot\" of relational graphs\nleads to neural networks with significantly improved predictive performance;\n(2) neural network's performance is approximately a smooth function of the\nclustering coefficient and average path length of its relational graph; (3) our\nfindings are consistent across many different tasks and datasets; (4) the sweet\nspot can be identified efficiently; (5) top-performing neural networks have\ngraph structure surprisingly similar to those of real biological neural\nnetworks. Our work opens new directions for the design of neural architectures\nand the understanding on neural networks in general.",
    "pdf_link": "http://arxiv.org/pdf/2007.06559v2.pdf",
    "authors": [
      "Jiaxuan You",
      "Jure Leskovec",
      "Kaiming He",
      "Saining Xie"
    ]
  },
  {
    "title": "Hybrid Quantum-Classical Neural Networks for Downlink Beamforming\n  Optimization",
    "text": "Hybrid Quantum-Classical Neural Networks for Downlink Beamforming\n  Optimization. This paper investigates quantum machine learning to optimize the beamforming\nin a multiuser multiple-input single-output downlink system. We aim to combine\nthe power of quantum neural networks and the success of classical deep neural\nnetworks to enhance the learning performance. Specifically, we propose two\nhybrid quantum-classical neural networks to maximize the sum rate of a downlink\nsystem. The first one proposes a quantum neural network employing parameterized\nquantum circuits that follows a classical convolutional neural network. The\nclassical neural network can be jointly trained with the quantum neural network\nor pre-trained leading to a fine-tuning transfer learning method. The second\none designs a quantum convolutional neural network to better extract features\nfollowed by a classical deep neural network. Our results demonstrate the\nfeasibility of the proposed hybrid neural networks, and reveal that the first\nmethod can achieve similar sum rate performance compared to a benchmark\nclassical neural network with significantly less training parameters; while the\nsecond method can achieve higher sum rate especially in presence of many users\nstill with less training parameters. The robustness of the proposed methods is\nverified using both software simulators and hardware emulators considering\nnoisy intermediate-scale quantum devices.",
    "pdf_link": "http://arxiv.org/pdf/2408.04747v1.pdf",
    "authors": [
      "Juping Zhang",
      "Gan Zheng",
      "Toshiaki Koike-Akino",
      "Kai-Kit Wong",
      "Fraser Burton"
    ]
  },
  {
    "title": "Cortex Neural Network: learning with Neural Network groups",
    "text": "Cortex Neural Network: learning with Neural Network groups. Neural Network has been successfully applied to many real-world problems,\nsuch as image recognition and machine translation. However, for the current\narchitecture of neural networks, it is hard to perform complex cognitive tasks,\nfor example, to process the image and audio inputs together. Cortex, as an\nimportant architecture in the brain, is important for animals to perform the\ncomplex cognitive task. We view the architecture of Cortex in the brain as a\nmissing part in the design of the current artificial neural network. In this\npaper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is\nan upper architecture of neural networks which motivated from cerebral cortex\nin the brain to handle different tasks in the same learning system. It is able\nto identify different tasks and solve them with different methods. In our\nimplementation, the Cortex Neural Network is able to process different\ncognitive tasks and perform reflection to get a higher accuracy. We provide a\nseries of experiments to examine the capability of the cortex architecture on\ntraditional neural networks. Our experiments proved its ability on the Cortex\nNeural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the\nsame time, which can promisingly reduce the loss by 40%.",
    "pdf_link": "http://arxiv.org/pdf/1804.03313v1.pdf",
    "authors": [
      "Liyao Gao"
    ]
  },
  {
    "title": "Parametrical Neural Networks and Some Other Similar Architectures",
    "text": "Parametrical Neural Networks and Some Other Similar Architectures. A review of works on associative neural networks accomplished during last\nfour years in the Institute of Optical Neural Technologies RAS is given. The\npresentation is based on description of parametrical neural networks (PNN). For\ntoday PNN have record recognizing characteristics (storage capacity, noise\nimmunity and speed of operation). Presentation of basic ideas and principles is\naccentuated.",
    "pdf_link": "http://arxiv.org/pdf/cs/0608073v1.pdf",
    "authors": [
      "Leonid B. Litinskii"
    ]
  },
  {
    "title": "Assessing Intelligence in Artificial Neural Networks",
    "text": "Assessing Intelligence in Artificial Neural Networks. The purpose of this work was to develop of metrics to assess network\narchitectures that balance neural network size and task performance. To this\nend, the concept of neural efficiency is introduced to measure neural layer\nutilization, and a second metric called artificial intelligence quotient (aIQ)\nwas created to balance neural network performance and neural network\nefficiency. To study aIQ and neural efficiency, two simple neural networks were\ntrained on MNIST: a fully connected network (LeNet-300-100) and a convolutional\nneural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32%\nless accurate but contained 30,912 times fewer parameters than the highest\naccuracy network. Both batch normalization and dropout layers were found to\nincrease neural efficiency. Finally, high aIQ networks are shown to be\nmemorization and overtraining resistant, capable of learning proper digit\nclassification with an accuracy of 92.51% even when 75% of the class labels are\nrandomized. These results demonstrate the utility of aIQ and neural efficiency\nas metrics for balancing network performance and size.",
    "pdf_link": "http://arxiv.org/pdf/2006.02909v1.pdf",
    "authors": [
      "Nicholas J. Schaub",
      "Nathan Hotaling"
    ]
  },
  {
    "title": "Rational Neural Network Controllers",
    "text": "Rational Neural Network Controllers. Neural networks have shown great success in many machine learning related\ntasks, due to their ability to act as general function approximators. Recent\nwork has demonstrated the effectiveness of neural networks in control systems\n(known as neural feedback loops), most notably by using a neural network as a\ncontroller. However, one of the big challenges of this approach is that neural\nnetworks have been shown to be sensitive to adversarial attacks. This means\nthat, unless they are designed properly, they are not an ideal candidate for\ncontrollers due to issues with robustness and uncertainty, which are pivotal\naspects of control systems. There has been initial work on robustness to both\nanalyse and design dynamical systems with neural network controllers. However,\none prominent issue with these methods is that they use existing neural network\narchitectures tailored for traditional machine learning tasks. These structures\nmay not be appropriate for neural network controllers and it is important to\nconsider alternative architectures. This paper considers rational neural\nnetworks and presents novel rational activation functions, which can be used\neffectively in robustness problems for neural feedback loops. Rational\nactivation functions are replaced by a general rational neural network\nstructure, which is convex in the neural network's parameters. A method is\nproposed to recover a stabilising controller from a Sum of Squares feasibility\ntest. This approach is then applied to a refined rational neural network which\nis more compatible with Sum of Squares programming. Numerical examples show\nthat this method can successfully recover stabilising rational neural network\ncontrollers for neural feedback loops with non-linear plants with noise and\nparametric uncertainty.",
    "pdf_link": "http://arxiv.org/pdf/2307.06287v1.pdf",
    "authors": [
      "Matthew Newton",
      "Antonis Papachristodoulou"
    ]
  },
  {
    "title": "Asymptotic Theory of Expectile Neural Networks",
    "text": "Asymptotic Theory of Expectile Neural Networks. Neural networks are becoming an increasingly important tool in applications.\nHowever, neural networks are not widely used in statistical genetics. In this\npaper, we propose a new neural networks method called expectile neural\nnetworks. When the size of parameter is too large, the standard maximum\nlikelihood procedures may not work. We use sieve method to constrain parameter\nspace. And we prove its consistency and normality under nonparametric\nregression framework.",
    "pdf_link": "http://arxiv.org/pdf/2011.01218v1.pdf",
    "authors": [
      "Jinghang Lin",
      "Xiaoxi Shen",
      "Qing Lu"
    ]
  },
  {
    "title": "Combining Recurrent and Convolutional Neural Networks for Relation\n  Classification",
    "text": "Combining Recurrent and Convolutional Neural Networks for Relation\n  Classification. This paper investigates two different neural architectures for the task of\nrelation classification: convolutional neural networks and recurrent neural\nnetworks. For both models, we demonstrate the effect of different architectural\nchoices. We present a new context representation for convolutional neural\nnetworks for relation classification (extended middle context). Furthermore, we\npropose connectionist bi-directional recurrent neural networks and introduce\nranking loss for their optimization. Finally, we show that combining\nconvolutional and recurrent neural networks using a simple voting scheme is\naccurate enough to improve results. Our neural models achieve state-of-the-art\nresults on the SemEval 2010 relation classification task.",
    "pdf_link": "http://arxiv.org/pdf/1605.07333v1.pdf",
    "authors": [
      "Ngoc Thang Vu",
      "Heike Adel",
      "Pankaj Gupta",
      "Hinrich Sch\u00fctze"
    ]
  },
  {
    "title": "A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices",
    "text": "A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices. Biological neural networks continue to inspire breakthroughs in neural\nnetwork performance. And yet, one key area of neural computation that has been\nunder-appreciated and under-investigated is biologically plausible,\nenergy-efficient spiking neural networks, whose potential is especially\nattractive for low-power, mobile, or otherwise hardware-constrained settings.\nWe present a literature review of recent developments in the interpretation,\noptimization, efficiency, and accuracy of spiking neural networks. Key\ncontributions include identification, discussion, and comparison of\ncutting-edge methods in spiking neural network optimization, energy-efficiency,\nand evaluation, starting from first principles so as to be accessible to new\npractitioners.",
    "pdf_link": "http://arxiv.org/pdf/2303.10780v2.pdf",
    "authors": [
      "Kai Malcolm",
      "Josue Casco-Rodriguez"
    ]
  },
  {
    "title": "Design and development of opto-neural processors for simulation of\n  neural networks trained in image detection for potential implementation in\n  hybrid robotics",
    "text": "Design and development of opto-neural processors for simulation of\n  neural networks trained in image detection for potential implementation in\n  hybrid robotics. Neural networks have been employed for a wide range of processing\napplications like image processing, motor control, object detection and many\nothers. Living neural networks offer advantages of lower power consumption,\nfaster processing, and biological realism. Optogenetics offers high spatial and\ntemporal control over biological neurons and presents potential in training\nlive neural networks. This work proposes a simulated living neural network\ntrained indirectly by backpropagating STDP based algorithms using precision\nactivation by optogenetics achieving accuracy comparable to traditional neural\nnetwork training algorithms.",
    "pdf_link": "http://arxiv.org/pdf/2401.10289v1.pdf",
    "authors": [
      "Sanjana Shetty"
    ]
  },
  {
    "title": "Convex Formulation of Overparameterized Deep Neural Networks",
    "text": "Convex Formulation of Overparameterized Deep Neural Networks. Analysis of over-parameterized neural networks has drawn significant\nattention in recentyears. It was shown that such systems behave like convex\nsystems under various restrictedsettings, such as for two-level neural\nnetworks, and when learning is only restricted locally inthe so-called neural\ntangent kernel space around specialized initializations. However, there areno\ntheoretical techniques that can analyze fully trained deep neural networks\nencountered inpractice. This paper solves this fundamental problem by\ninvestigating such overparameterizeddeep neural networks when fully trained. We\ngeneralize a new technique called neural feature repopulation, originally\nintroduced in (Fang et al., 2019a) for two-level neural networks, to analyze\ndeep neural networks. It is shown that under suitable representations,\noverparameterized deep neural networks are inherently convex, and when\noptimized, the system can learn effective features suitable for the underlying\nlearning task under mild conditions. This new analysis is consistent with\nempirical observations that deep neural networks are capable of learning\nefficient feature representations. Therefore, the highly unexpected result of\nthis paper can satisfactorily explain the practical success of deep neural\nnetworks. Empirical studies confirm that predictions of our theory are\nconsistent with results observed in practice.",
    "pdf_link": "http://arxiv.org/pdf/1911.07626v1.pdf",
    "authors": [
      "Cong Fang",
      "Yihong Gu",
      "Weizhong Zhang",
      "Tong Zhang"
    ]
  },
  {
    "title": "Approximate Bisimulation Relations for Neural Networks and Application\n  to Assured Neural Network Compression",
    "text": "Approximate Bisimulation Relations for Neural Networks and Application\n  to Assured Neural Network Compression. In this paper, we propose a concept of approximate bisimulation relation for\nfeedforward neural networks. In the framework of approximate bisimulation\nrelation, a novel neural network merging method is developed to compute the\napproximate bisimulation error between two neural networks based on\nreachability analysis of neural networks. The developed method is able to\nquantitatively measure the distance between the outputs of two neural networks\nwith the same inputs. Then, we apply the approximate bisimulation relation\nresults to perform neural networks model reduction and compute the compression\nprecision, i.e., assured neural networks compression. At last, using the\nassured neural network compression, we accelerate the verification processes of\nACAS Xu neural networks to illustrate the effectiveness and advantages of our\nproposed approximate bisimulation approach.",
    "pdf_link": "http://arxiv.org/pdf/2202.01214v1.pdf",
    "authors": [
      "Weiming Xiang",
      "Zhongzhu Shao"
    ]
  },
  {
    "title": "Optimal rates of approximation by shallow ReLU$^k$ neural networks and\n  applications to nonparametric regression",
    "text": "Optimal rates of approximation by shallow ReLU$^k$ neural networks and\n  applications to nonparametric regression. We study the approximation capacity of some variation spaces corresponding to\nshallow ReLU$^k$ neural networks. It is shown that sufficiently smooth\nfunctions are contained in these spaces with finite variation norms. For\nfunctions with less smoothness, the approximation rates in terms of the\nvariation norm are established. Using these results, we are able to prove the\noptimal approximation rates in terms of the number of neurons for shallow\nReLU$^k$ neural networks. It is also shown how these results can be used to\nderive approximation bounds for deep neural networks and convolutional neural\nnetworks (CNNs). As applications, we study convergence rates for nonparametric\nregression using three ReLU neural network models: shallow neural network,\nover-parameterized neural network, and CNN. In particular, we show that shallow\nneural networks can achieve the minimax optimal rates for learning H\\\"older\nfunctions, which complements recent results for deep neural networks. It is\nalso proven that over-parameterized (deep or shallow) neural networks can\nachieve nearly optimal rates for nonparametric regression.",
    "pdf_link": "http://arxiv.org/pdf/2304.01561v3.pdf",
    "authors": [
      "Yunfei Yang",
      "Ding-Xuan Zhou"
    ]
  },
  {
    "title": "Understanding Vector-Valued Neural Networks and Their Relationship with\n  Real and Hypercomplex-Valued Neural Networks",
    "text": "Understanding Vector-Valued Neural Networks and Their Relationship with\n  Real and Hypercomplex-Valued Neural Networks. Despite the many successful applications of deep learning models for\nmultidimensional signal and image processing, most traditional neural networks\nprocess data represented by (multidimensional) arrays of real numbers. The\nintercorrelation between feature channels is usually expected to be learned\nfrom the training data, requiring numerous parameters and careful training. In\ncontrast, vector-valued neural networks are conceived to process arrays of\nvectors and naturally consider the intercorrelation between feature channels.\nConsequently, they usually have fewer parameters and often undergo more robust\ntraining than traditional neural networks. This paper aims to present a broad\nframework for vector-valued neural networks, referred to as V-nets. In this\ncontext, hypercomplex-valued neural networks are regarded as vector-valued\nmodels with additional algebraic properties. Furthermore, this paper explains\nthe relationship between vector-valued and traditional neural networks.\nPrecisely, a vector-valued neural network can be obtained by placing\nrestrictions on a real-valued model to consider the intercorrelation between\nfeature channels. Finally, we show how V-nets, including hypercomplex-valued\nneural networks, can be implemented in current deep-learning libraries as\nreal-valued networks.",
    "pdf_link": "http://arxiv.org/pdf/2309.07716v2.pdf",
    "authors": [
      "Marcos Eduardo Valle"
    ]
  },
  {
    "title": "One weird trick for parallelizing convolutional neural networks",
    "text": "One weird trick for parallelizing convolutional neural networks. I present a new way to parallelize the training of convolutional neural\nnetworks across multiple GPUs. The method scales significantly better than all\nalternatives when applied to modern convolutional neural networks.",
    "pdf_link": "http://arxiv.org/pdf/1404.5997v2.pdf",
    "authors": [
      "Alex Krizhevsky"
    ]
  },
  {
    "title": "Nonlinear Systems Identification Using Deep Dynamic Neural Networks",
    "text": "Nonlinear Systems Identification Using Deep Dynamic Neural Networks. Neural networks are known to be effective function approximators. Recently,\ndeep neural networks have proven to be very effective in pattern recognition,\nclassification tasks and human-level control to model highly nonlinear\nrealworld systems. This paper investigates the effectiveness of deep neural\nnetworks in the modeling of dynamical systems with complex behavior. Three deep\nneural network structures are trained on sequential data, and we investigate\nthe effectiveness of these networks in modeling associated characteristics of\nthe underlying dynamical systems. We carry out similar evaluations on select\npublicly available system identification datasets. We demonstrate that deep\nneural networks are effective model estimators from input-output data",
    "pdf_link": "http://arxiv.org/pdf/1610.01439v1.pdf",
    "authors": [
      "Olalekan Ogunmolu",
      "Xuejun Gu",
      "Steve Jiang",
      "Nicholas Gans"
    ]
  },
  {
    "title": "Geometric Decomposition of Feed Forward Neural Networks",
    "text": "Geometric Decomposition of Feed Forward Neural Networks. There have been several attempts to mathematically understand neural networks\nand many more from biological and computational perspectives. The field has\nexploded in the last decade, yet neural networks are still treated much like a\nblack box. In this work we describe a structure that is inherent to a feed\nforward neural network. This will provide a framework for future work on neural\nnetworks to improve training algorithms, compute the homology of the network,\nand other applications. Our approach takes a more geometric point of view and\nis unlike other attempts to mathematically understand neural networks that rely\non a functional perspective.",
    "pdf_link": "http://arxiv.org/pdf/1612.02522v1.pdf",
    "authors": [
      "Sven Cattell"
    ]
  },
  {
    "title": "Neural Networks Architecture Evaluation in a Quantum Computer",
    "text": "Neural Networks Architecture Evaluation in a Quantum Computer. In this work, we propose a quantum algorithm to evaluate neural networks\narchitectures named Quantum Neural Network Architecture Evaluation (QNNAE). The\nproposed algorithm is based on a quantum associative memory and the learning\nalgorithm for artificial neural networks. Unlike conventional algorithms for\nevaluating neural network architectures, QNNAE does not depend on\ninitialization of weights. The proposed algorithm has a binary output and\nresults in 0 with probability proportional to the performance of the network.\nAnd its computational cost is equal to the computational cost to train a neural\nnetwork.",
    "pdf_link": "http://arxiv.org/pdf/1711.04759v1.pdf",
    "authors": [
      "Adenilton Jos\u00e9 da Silva",
      "Rodolfo Luan F. de Oliveira"
    ]
  },
  {
    "title": "Building Compact and Robust Deep Neural Networks with Toeplitz Matrices",
    "text": "Building Compact and Robust Deep Neural Networks with Toeplitz Matrices. Deep neural networks are state-of-the-art in a wide variety of tasks,\nhowever, they exhibit important limitations which hinder their use and\ndeployment in real-world applications. When developing and training neural\nnetworks, the accuracy should not be the only concern, neural networks must\nalso be cost-effective and reliable. Although accurate, large neural networks\noften lack these properties. This thesis focuses on the problem of training\nneural networks which are not only accurate but also compact, easy to train,\nreliable and robust to adversarial examples. To tackle these problems, we\nleverage the properties of structured matrices from the Toeplitz family to\nbuild compact and secure neural networks.",
    "pdf_link": "http://arxiv.org/pdf/2109.00959v1.pdf",
    "authors": [
      "Alexandre Araujo"
    ]
  },
  {
    "title": "Application of Neural Network in Optimization of Chemical Process",
    "text": "Application of Neural Network in Optimization of Chemical Process. Artificial neural network (ANN) has been widely used due to its strong\nnonlinear mapping ability, fault tolerance and self-learning ability. This\narticle summarizes the development history of artificial neural networks,\nintroduces three common neural network types, BP neural network, RBF neural\nnetwork and convolutional neural network, and focuses on the practical\napplication in chemical process optimization, especially the results achieved\nin multi-objective control optimization and process parameter improvement.",
    "pdf_link": "http://arxiv.org/pdf/2110.04942v1.pdf",
    "authors": [
      "Fei Liang",
      "Taowen Zhang"
    ]
  },
  {
    "title": "Compact Matrix Quantum Group Equivariant Neural Networks",
    "text": "Compact Matrix Quantum Group Equivariant Neural Networks. We derive the existence of a new type of neural network, called a compact\nmatrix quantum group equivariant neural network, that learns from data that has\nan underlying quantum symmetry. We apply the Woronowicz formulation of\nTannaka-Krein duality to characterise the weight matrices that appear in these\nneural networks for any easy compact matrix quantum group. We show that compact\nmatrix quantum group equivariant neural networks contain, as a subclass, all\ncompact matrix group equivariant neural networks. Moreover, we obtain\ncharacterisations of the weight matrices for many compact matrix group\nequivariant neural networks that have not previously appeared in the machine\nlearning literature.",
    "pdf_link": "http://arxiv.org/pdf/2311.06358v1.pdf",
    "authors": [
      "Edward Pearce-Crump"
    ]
  },
  {
    "title": "Universal Approximation Theorem for Vector- and Hypercomplex-Valued\n  Neural Networks",
    "text": "Universal Approximation Theorem for Vector- and Hypercomplex-Valued\n  Neural Networks. The universal approximation theorem states that a neural network with one\nhidden layer can approximate continuous functions on compact sets with any\ndesired precision. This theorem supports using neural networks for various\napplications, including regression and classification tasks. Furthermore, it is\nvalid for real-valued neural networks and some hypercomplex-valued neural\nnetworks such as complex-, quaternion-, tessarine-, and Clifford-valued neural\nnetworks. However, hypercomplex-valued neural networks are a type of\nvector-valued neural network defined on an algebra with additional algebraic or\ngeometric properties. This paper extends the universal approximation theorem\nfor a wide range of vector-valued neural networks, including\nhypercomplex-valued models as particular instances. Precisely, we introduce the\nconcept of non-degenerate algebra and state the universal approximation theorem\nfor neural networks defined on such algebras.",
    "pdf_link": "http://arxiv.org/pdf/2401.02277v2.pdf",
    "authors": [
      "Marcos Eduardo Valle",
      "Wington L. Vital",
      "Guilherme Vieira"
    ]
  },
  {
    "title": "Detecting Neural Trojans Through Merkle Trees",
    "text": "Detecting Neural Trojans Through Merkle Trees. Deep neural networks are utilized in a growing number of industries. Much of\nthe current literature focuses on the applications of deep neural networks\nwithout discussing the security of the network itself. One security issue\nfacing deep neural networks is neural trojans. Through a neural trojan, a\nmalicious actor may force the deep neural network to act in unintended ways.\nSeveral potential defenses have been proposed, but they are computationally\nexpensive, complex, or unusable in commercial applications. We propose Merkle\ntrees as a novel way to detect and isolate neural trojans.",
    "pdf_link": "http://arxiv.org/pdf/2306.05368v1.pdf",
    "authors": [
      "Joshua Strubel"
    ]
  },
  {
    "title": "Performance Analysis Of Neural Network Models For Oxazolines And\n  Oxazoles Derivatives Descriptor Dataset",
    "text": "Performance Analysis Of Neural Network Models For Oxazolines And\n  Oxazoles Derivatives Descriptor Dataset. Neural networks have been used successfully to a broad range of areas such as\nbusiness, data mining, drug discovery and biology. In medicine, neural networks\nhave been applied widely in medical diagnosis, detection and evaluation of new\ndrugs and treatment cost estimation. In addition, neural networks have begin\npractice in data mining strategies for the aim of prediction, knowledge\ndiscovery. This paper will present the application of neural networks for the\nprediction and analysis of antitubercular activity of Oxazolines and Oxazoles\nderivatives. This study presents techniques based on the development of Single\nhidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural\nnetwork (GDBPNN), Gradient Descent Back propagation with momentum neural\nnetwork (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN)\nand Quantile regression neural network (QRNN) of artificial neural network\n(ANN) models Here, we comparatively evaluate the performance of five neural\nnetwork techniques. The evaluation of the efficiency of each model by ways of\nbenchmark experiments is an accepted application. Cross-validation and\nresampling techniques are commonly used to derive point estimates of the\nperformances which are compared to identify methods with good properties.\nPredictive accuracy was evaluated using the root mean squared error (RMSE),\nCoefficient determination(???), mean absolute error(MAE), mean percentage\nerror(MPE) and relative square error(RSE). We found that all five neural\nnetwork models were able to produce feasible models. QRNN model is outperforms\nwith all statistical tests amongst other four models.",
    "pdf_link": "http://arxiv.org/pdf/1312.2853v1.pdf",
    "authors": [
      "Doreswamy",
      "Chanabasayya . M. Vastrad"
    ]
  },
  {
    "title": "Why Quantization Improves Generalization: NTK of Binary Weight Neural\n  Networks",
    "text": "Why Quantization Improves Generalization: NTK of Binary Weight Neural\n  Networks. Quantized neural networks have drawn a lot of attention as they reduce the\nspace and computational complexity during the inference. Moreover, there has\nbeen folklore that quantization acts as an implicit regularizer and thus can\nimprove the generalizability of neural networks, yet no existing work\nformalizes this interesting folklore. In this paper, we take the binary weights\nin a neural network as random variables under stochastic rounding, and study\nthe distribution propagation over different layers in the neural network. We\npropose a quasi neural network to approximate the distribution propagation,\nwhich is a neural network with continuous parameters and smooth activation\nfunction. We derive the neural tangent kernel (NTK) for this quasi neural\nnetwork, and show that the eigenvalue of NTK decays at approximately\nexponential rate, which is comparable to that of Gaussian kernel with\nrandomized scale. This in turn indicates that the Reproducing Kernel Hilbert\nSpace (RKHS) of a binary weight neural network covers a strict subset of\nfunctions compared with the one with real value weights. We use experiments to\nverify that the quasi neural network we proposed can well approximate binary\nweight neural network. Furthermore, binary weight neural network gives a lower\ngeneralization gap compared with real value weight neural network, which is\nsimilar to the difference between Gaussian kernel and Laplace kernel.",
    "pdf_link": "http://arxiv.org/pdf/2206.05916v1.pdf",
    "authors": [
      "Kaiqi Zhang",
      "Ming Yin",
      "Yu-Xiang Wang"
    ]
  },
  {
    "title": "Bayesian Neural Networks: Essentials",
    "text": "Bayesian Neural Networks: Essentials. Bayesian neural networks utilize probabilistic layers that capture\nuncertainty over weights and activations, and are trained using Bayesian\ninference. Since these probabilistic layers are designed to be drop-in\nreplacement of their deterministic counter parts, Bayesian neural networks\nprovide a direct and natural way to extend conventional deep neural networks to\nsupport probabilistic deep learning. However, it is nontrivial to understand,\ndesign and train Bayesian neural networks due to their complexities. We discuss\nthe essentials of Bayesian neural networks including duality (deep neural\nnetworks, probabilistic models), approximate Bayesian inference, Bayesian\npriors, Bayesian posteriors, and deep variational learning. We use TensorFlow\nProbability APIs and code examples for illustration. The main problem with\nBayesian neural networks is that the architecture of deep neural networks makes\nit quite redundant, and costly, to account for uncertainty for a large number\nof successive layers. Hybrid Bayesian neural networks, which use few\nprobabilistic layers judicially positioned in the networks, provide a practical\nsolution.",
    "pdf_link": "http://arxiv.org/pdf/2106.13594v1.pdf",
    "authors": [
      "Daniel T. Chang"
    ]
  },
  {
    "title": "Fourier Neural Networks for Function Approximation",
    "text": "Fourier Neural Networks for Function Approximation. The success of Neural networks in providing miraculous results when applied\nto a wide variety of tasks is astonishing. Insight in the working can be\nobtained by studying the universal approximation property of neural networks.\nIt is proved extensively that neural networks are universal approximators.\nFurther it is proved that deep Neural networks are better approximators. It is\nspecifically proved that for a narrow neural network to approximate a function\nwhich is otherwise implemented by a deep Neural network, the network take\nexponentially large number of neurons. In this work, we have implemented\nexisting methodologies for a variety of synthetic functions and identified\ntheir deficiencies. Further, we examined that Fourier neural network is able to\nperform fairly good with only two layers in the neural network. A modified\nFourier Neural network which has sinusoidal activation and two hidden layer is\nproposed and the results are tabulated.",
    "pdf_link": "http://arxiv.org/pdf/2111.08438v1.pdf",
    "authors": [
      "R Subhash Chandra Bose",
      "Kakarla Yaswanth"
    ]
  },
  {
    "title": "Genetic cellular neural networks for generating three-dimensional\n  geometry",
    "text": "Genetic cellular neural networks for generating three-dimensional\n  geometry. There are a number of ways to procedurally generate interesting\nthree-dimensional shapes, and a method where a cellular neural network is\ncombined with a mesh growth algorithm is presented here. The aim is to create a\nshape from a genetic code in such a way that a crude search can find\ninteresting shapes. Identical neural networks are placed at each vertex of a\nmesh which can communicate with neural networks on neighboring vertices. The\noutput of the neural networks determine how the mesh grows, allowing\ninteresting shapes to be produced emergently, mimicking some of the complexity\nof biological organism development. Since the neural networks' parameters can\nbe freely mutated, the approach is amenable for use in a genetic algorithm.",
    "pdf_link": "http://arxiv.org/pdf/1603.08551v1.pdf",
    "authors": [
      "Hugo Martay"
    ]
  },
  {
    "title": "Survey of Dropout Methods for Deep Neural Networks",
    "text": "Survey of Dropout Methods for Deep Neural Networks. Dropout methods are a family of stochastic techniques used in neural network\ntraining or inference that have generated significant research interest and are\nwidely used in practice. They have been successfully applied in neural network\nregularization, model compression, and in measuring the uncertainty of neural\nnetwork outputs. While original formulated for dense neural network layers,\nrecent advances have made dropout methods also applicable to convolutional and\nrecurrent neural network layers. This paper summarizes the history of dropout\nmethods, their various applications, and current areas of research interest.\nImportant proposed methods are described in additional detail.",
    "pdf_link": "http://arxiv.org/pdf/1904.13310v2.pdf",
    "authors": [
      "Alex Labach",
      "Hojjat Salehinejad",
      "Shahrokh Valaee"
    ]
  },
  {
    "title": "General Regression Neural Networks, Radial Basis Function Neural\n  Networks, Support Vector Machines, and Feedforward Neural Networks",
    "text": "General Regression Neural Networks, Radial Basis Function Neural\n  Networks, Support Vector Machines, and Feedforward Neural Networks. The aim of this project is to develop a code to discover the optimal sigma\nvalue that maximum the F1 score and the optimal sigma value that maximizes the\naccuracy and to find out if they are the same. Four algorithms which can be\nused to solve this problem are: Genetic Regression Neural Networks (GRNNs),\nRadial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines\n(SVMs) and Feedforward Neural Network (FFNNs).",
    "pdf_link": "http://arxiv.org/pdf/1911.07115v1.pdf",
    "authors": [
      "Alison Jenkins",
      "Vinika Gupta",
      "Mary Lenoir"
    ]
  },
  {
    "title": "On neural network kernels and the storage capacity problem",
    "text": "On neural network kernels and the storage capacity problem. In this short note, we reify the connection between work on the storage\ncapacity problem in wide two-layer treelike neural networks and the\nrapidly-growing body of literature on kernel limits of wide neural networks.\nConcretely, we observe that the \"effective order parameter\" studied in the\nstatistical mechanics literature is exactly equivalent to the infinite-width\nNeural Network Gaussian Process Kernel. This correspondence connects the\nexpressivity and trainability of wide two-layer neural networks.",
    "pdf_link": "http://arxiv.org/pdf/2201.04669v1.pdf",
    "authors": [
      "Jacob A. Zavatone-Veth",
      "Cengiz Pehlevan"
    ]
  },
  {
    "title": "Unary Coding for Neural Network Learning",
    "text": "Unary Coding for Neural Network Learning. This paper presents some properties of unary coding of significance for\nbiological learning and instantaneously trained neural networks.",
    "pdf_link": "http://arxiv.org/pdf/1009.4495v1.pdf",
    "authors": [
      "Subhash Kak"
    ]
  },
  {
    "title": "Deep Neural Networks - A Brief History",
    "text": "Deep Neural Networks - A Brief History. Introduction to deep neural networks and their history.",
    "pdf_link": "http://arxiv.org/pdf/1701.05549v1.pdf",
    "authors": [
      "Krzysztof J. Cios"
    ]
  },
  {
    "title": "GPU Acceleration of Sparse Neural Networks",
    "text": "GPU Acceleration of Sparse Neural Networks. In this paper, we use graphics processing units(GPU) to accelerate sparse and\narbitrary structured neural networks. Sparse networks have nodes in the network\nthat are not fully connected with nodes in preceding and following layers, and\narbitrary structure neural networks have different number of nodes in each\nlayers. Sparse Neural networks with arbitrary structures are generally created\nin the processes like neural network pruning and evolutionary machine learning\nstrategies. We show that we can gain significant speedup for full activation of\nsuch neural networks using graphical processing units. We do a prepossessing\nstep to determine dependency groups for all the nodes in a network, and use\nthat information to guide the progression of activation in the neural network.\nThen we compute activation for each nodes in its own separate thread in the\nGPU, which allows for massive parallelization. We use CUDA framework to\nimplement our approach and compare the results of sequential and GPU\nimplementations. Our results show that the activation of sparse neural networks\nlends very well to GPU acceleration and can help speed up machine learning\nstrategies which generate such networks or other processes that have similar\nstructure.",
    "pdf_link": "http://arxiv.org/pdf/2005.04347v1.pdf",
    "authors": [
      "Aavaas Gajurel",
      "Sushil J. Louis",
      "Frederick C Harris"
    ]
  },
  {
    "title": "Neural Network Pruning as Spectrum Preserving Process",
    "text": "Neural Network Pruning as Spectrum Preserving Process. Neural networks have achieved remarkable performance in various application\ndomains. Nevertheless, a large number of weights in pre-trained deep neural\nnetworks prohibit them from being deployed on smartphones and embedded systems.\nIt is highly desirable to obtain lightweight versions of neural networks for\ninference in edge devices. Many cost-effective approaches were proposed to\nprune dense and convolutional layers that are common in deep neural networks\nand dominant in the parameter space. However, a unified theoretical foundation\nfor the problem mostly is missing. In this paper, we identify the close\nconnection between matrix spectrum learning and neural network training for\ndense and convolutional layers and argue that weight pruning is essentially a\nmatrix sparsification process to preserve the spectrum. Based on the analysis,\nwe also propose a matrix sparsification algorithm tailored for neural network\npruning that yields better pruning result. We carefully design and conduct\nexperiments to support our arguments. Hence we provide a consolidated viewpoint\nfor neural network pruning and enhance the interpretability of deep neural\nnetworks by identifying and preserving the critical neural weights.",
    "pdf_link": "http://arxiv.org/pdf/2307.08982v1.pdf",
    "authors": [
      "Shibo Yao",
      "Dantong Yu",
      "Ioannis Koutis"
    ]
  },
  {
    "title": "On Hiding Neural Networks Inside Neural Networks",
    "text": "On Hiding Neural Networks Inside Neural Networks. Modern neural networks often contain significantly more parameters than the\nsize of their training data. We show that this excess capacity provides an\nopportunity for embedding secret machine learning models within a trained\nneural network. Our novel framework hides the existence of a secret neural\nnetwork with arbitrary desired functionality within a carrier network. We prove\ntheoretically that the secret network's detection is computationally infeasible\nand demonstrate empirically that the carrier network does not compromise the\nsecret network's disguise. Our paper introduces a previously unknown\nsteganographic technique that can be exploited by adversaries if left\nunchecked.",
    "pdf_link": "http://arxiv.org/pdf/2002.10078v3.pdf",
    "authors": [
      "Chuan Guo",
      "Ruihan Wu",
      "Kilian Q. Weinberger"
    ]
  },
  {
    "title": "A New Constructive Method to Optimize Neural Network Architecture and\n  Generalization",
    "text": "A New Constructive Method to Optimize Neural Network Architecture and\n  Generalization. In this paper, after analyzing the reasons of poor generalization and\noverfitting in neural networks, we consider some noise data as a singular value\nof a continuous function - jump discontinuity point. The continuous part can be\napproximated with the simplest neural networks, which have good generalization\nperformance and optimal network architecture, by traditional algorithms such as\nconstructive algorithm for feed-forward neural networks with incremental\ntraining, BP algorithm, ELM algorithm, various constructive algorithm, RBF\napproximation and SVM. At the same time, we will construct RBF neural networks\nto fit the singular value with every error in, and we prove that a function\nwith jumping discontinuity points can be approximated by the simplest neural\nnetworks with a decay RBF neural networks in by each error, and a function with\njumping discontinuity point can be constructively approximated by a decay RBF\nneural networks in by each error and the constructive part have no\ngeneralization influence to the whole machine learning system which will\noptimize neural network architecture and generalization performance, reduce the\noverfitting phenomenon by avoid fitting the noisy data.",
    "pdf_link": "http://arxiv.org/pdf/1302.0324v1.pdf",
    "authors": [
      "Hou Muzhou",
      "Moon Ho Lee"
    ]
  },
  {
    "title": "Deep physical neural networks enabled by a backpropagation algorithm for\n  arbitrary physical systems",
    "text": "Deep physical neural networks enabled by a backpropagation algorithm for\n  arbitrary physical systems. Deep neural networks have become a pervasive tool in science and engineering.\nHowever, modern deep neural networks' growing energy requirements now\nincreasingly limit their scaling and broader use. We propose a radical\nalternative for implementing deep neural network models: Physical Neural\nNetworks. We introduce a hybrid physical-digital algorithm called Physics-Aware\nTraining to efficiently train sequences of controllable physical systems to act\nas deep neural networks. This method automatically trains the functionality of\nany sequence of real physical systems, directly, using backpropagation, the\nsame technique used for modern deep neural networks. To illustrate their\ngenerality, we demonstrate physical neural networks with three diverse physical\nsystems-optical, mechanical, and electrical. Physical neural networks may\nfacilitate unconventional machine learning hardware that is orders of magnitude\nfaster and more energy efficient than conventional electronic processors.",
    "pdf_link": "http://arxiv.org/pdf/2104.13386v1.pdf",
    "authors": [
      "Logan G. Wright",
      "Tatsuhiro Onodera",
      "Martin M. Stein",
      "Tianyu Wang",
      "Darren T. Schachter",
      "Zoey Hu",
      "Peter L. McMahon"
    ]
  },
  {
    "title": "Understanding Weight Similarity of Neural Networks via Chain\n  Normalization Rule and Hypothesis-Training-Testing",
    "text": "Understanding Weight Similarity of Neural Networks via Chain\n  Normalization Rule and Hypothesis-Training-Testing. We present a weight similarity measure method that can quantify the weight\nsimilarity of non-convex neural networks. To understand the weight similarity\nof different trained models, we propose to extract the feature representation\nfrom the weights of neural networks. We first normalize the weights of neural\nnetworks by introducing a chain normalization rule, which is used for weight\nrepresentation learning and weight similarity measure. We extend the\ntraditional hypothesis-testing method to a hypothesis-training-testing\nstatistical inference method to validate the hypothesis on the weight\nsimilarity of neural networks. With the chain normalization rule and the new\nstatistical inference, we study the weight similarity measure on Multi-Layer\nPerceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural\nNetwork (RNN), and find that the weights of an identical neural network\noptimized with the Stochastic Gradient Descent (SGD) algorithm converge to a\nsimilar local solution in a metric space. The weight similarity measure\nprovides more insight into the local solutions of neural networks. Experiments\non several datasets consistently validate the hypothesis of weight similarity\nmeasure.",
    "pdf_link": "http://arxiv.org/pdf/2208.04369v1.pdf",
    "authors": [
      "Guangcong Wang",
      "Guangrun Wang",
      "Wenqi Liang",
      "Jianhuang Lai"
    ]
  },
  {
    "title": "Consistency of Neural Networks with Regularization",
    "text": "Consistency of Neural Networks with Regularization. Neural networks have attracted a lot of attention due to its success in\napplications such as natural language processing and computer vision. For large\nscale data, due to the tremendous number of parameters in neural networks,\noverfitting is an issue in training neural networks. To avoid overfitting, one\ncommon approach is to penalize the parameters especially the weights in neural\nnetworks. Although neural networks has demonstrated its advantages in many\napplications, the theoretical foundation of penalized neural networks has not\nbeen well-established. Our goal of this paper is to propose the general\nframework of neural networks with regularization and prove its consistency.\nUnder certain conditions, the estimated neural network will converge to true\nunderlying function as the sample size increases. The method of sieves and the\ntheory on minimal neural networks are used to overcome the issue of\nunidentifiability for the parameters. Two types of activation functions:\nhyperbolic tangent function(Tanh) and rectified linear unit(ReLU) have been\ntaken into consideration. Simulations have been conducted to verify the\nvalidation of theorem of consistency.",
    "pdf_link": "http://arxiv.org/pdf/2207.01538v1.pdf",
    "authors": [
      "Xiaoxi Shen",
      "Jinghang Lin"
    ]
  },
  {
    "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
    "text": "Graph Metanetworks for Processing Diverse Neural Architectures. Neural networks efficiently encode learned information within their\nparameters. Consequently, many tasks can be unified by treating neural networks\nthemselves as input data. When doing so, recent studies demonstrated the\nimportance of accounting for the symmetries and geometry of parameter spaces.\nHowever, those works developed architectures tailored to specific networks such\nas MLPs and CNNs without normalization layers, and generalizing such\narchitectures to other types of networks can be challenging. In this work, we\novercome these challenges by building new metanetworks - neural networks that\ntake weights from other neural networks as input. Put simply, we carefully\nbuild graphs representing the input neural networks and process the graphs\nusing graph neural networks. Our approach, Graph Metanetworks (GMNs),\ngeneralizes to neural architectures where competing methods struggle, such as\nmulti-head attention layers, normalization layers, convolutional layers, ResNet\nblocks, and group-equivariant linear layers. We prove that GMNs are expressive\nand equivariant to parameter permutation symmetries that leave the input neural\nnetwork functions unchanged. We validate the effectiveness of our method on\nseveral metanetwork tasks over diverse neural network architectures.",
    "pdf_link": "http://arxiv.org/pdf/2312.04501v2.pdf",
    "authors": [
      "Derek Lim",
      "Haggai Maron",
      "Marc T. Law",
      "Jonathan Lorraine",
      "James Lucas"
    ]
  },
  {
    "title": "Bayesian Learning of Neural Networks for Signal/Background\n  Discrimination in Particle Physics",
    "text": "Bayesian Learning of Neural Networks for Signal/Background\n  Discrimination in Particle Physics. Neural networks are used extensively in classification problems in particle\nphysics research. Since the training of neural networks can be viewed as a\nproblem of inference, Bayesian learning of neural networks can provide more\noptimal and robust results than conventional learning methods. We have\ninvestigated the use of Bayesian neural networks for signal/background\ndiscrimination in the search for second generation leptoquarks at the Tevatron,\nas an example. We present a comparison of the results obtained from the\nconventional training of feedforward neural networks and networks trained with\nBayesian methods.",
    "pdf_link": "http://arxiv.org/pdf/0707.0930v1.pdf",
    "authors": [
      "Michael Pogwizd",
      "Laura Jane Elgass",
      "Pushpalatha C. Bhat"
    ]
  },
  {
    "title": "Deep Neural Networks for Pattern Recognition",
    "text": "Deep Neural Networks for Pattern Recognition. In the field of pattern recognition research, the method of using deep neural\nnetworks based on improved computing hardware recently attracted attention\nbecause of their superior accuracy compared to conventional methods. Deep\nneural networks simulate the human visual system and achieve human equivalent\naccuracy in image classification, object detection, and segmentation. This\nchapter introduces the basic structure of deep neural networks that simulate\nhuman neural networks. Then we identify the operational processes and\napplications of conditional generative adversarial networks, which are being\nactively researched based on the bottom-up and top-down mechanisms, the most\nimportant functions of the human visual perception process. Finally, recent\ndevelopments in training strategies for effective learning of complex deep\nneural networks are addressed.",
    "pdf_link": "http://arxiv.org/pdf/1809.09645v1.pdf",
    "authors": [
      "Kyongsik Yun",
      "Alexander Huyen",
      "Thomas Lu"
    ]
  },
  {
    "title": "Evidence, Definitions and Algorithms regarding the Existence of\n  Cohesive-Convergence Groups in Neural Network Optimization",
    "text": "Evidence, Definitions and Algorithms regarding the Existence of\n  Cohesive-Convergence Groups in Neural Network Optimization. Understanding the convergence process of neural networks is one of the most\ncomplex and crucial issues in the field of machine learning. Despite the close\nassociation of notable successes in this domain with the convergence of\nartificial neural networks, this concept remains predominantly theoretical. In\nreality, due to the non-convex nature of the optimization problems that\nartificial neural networks tackle, very few trained networks actually achieve\nconvergence. To expand recent research efforts on artificial-neural-network\nconvergence, this paper will discuss a different approach based on observations\nof cohesive-convergence groups emerging during the optimization process of an\nartificial neural network.",
    "pdf_link": "http://arxiv.org/pdf/2403.05610v1.pdf",
    "authors": [
      "Thien An L. Nguyen"
    ]
  },
  {
    "title": "Hybrid deep neural network based prediction method for unsteady flows\n  with moving boundaries",
    "text": "Hybrid deep neural network based prediction method for unsteady flows\n  with moving boundaries. A novel hybrid deep neural network architecture is designed to capture the\nspatial-temporal features of unsteady flows around moving boundaries directly\nfrom high-dimensional unsteady flow fields data. The hybrid deep neural network\nis constituted by the convolutional neural network (CNN), improved\nconvolutional Long-Short Term Memory neural network (ConvLSTM) and\ndeconvolutional neural network (DeCNN). Flow fields at future time step can be\npredicted through flow fields by previous time steps and boundary positions at\nthose steps by the novel hybrid deep neural network. Unsteady wake flows around\na forced oscillation cylinder with various amplitudes are calculated to\nestablish the datasets as training samples for training the hybrid deep neural\nnetworks. The trained hybrid deep neural networks are then tested by predicting\nthe unsteady flow fields around a forced oscillation cylinder with new\namplitude. The effect of neural network structure parameters on prediction\naccuracy was analyzed. The hybrid deep neural network, constituted by the best\nparameter combination, is used to predict the flow fields in the future time.\nThe predicted flow fields are in good agreement with those calculated directly\nby computational fluid dynamic solver, which means that this kind of deep\nneural network can capture accurate spatial-temporal information from the\nspatial-temporal series of unsteady flows around moving boundaries. The result\nshows the potential capability of this kind novel hybrid deep neural network in\nflow control for vibrating cylinder, where the fast calculation of\nhigh-dimensional nonlinear unsteady flow around moving boundaries is needed.",
    "pdf_link": "http://arxiv.org/pdf/2006.00690v1.pdf",
    "authors": [
      "Renkun Han",
      "Zhong Zhang",
      "Yixing Wang",
      "Ziyang Liu",
      "Yang Zhang",
      "Gang Chen"
    ]
  },
  {
    "title": "Neural network learning dynamics in a path integral framework",
    "text": "Neural network learning dynamics in a path integral framework. A path-integral formalism is proposed for studying the dynamical evolution in\ntime of patterns in an artificial neural network in the presence of noise. An\neffective cost function is constructed which determines the unique global\nminimum of the neural network system. The perturbative method discussed also\nprovides a way for determining the storage capacity of the network.",
    "pdf_link": "http://arxiv.org/pdf/cond-mat/0308503v1.pdf",
    "authors": [
      "J. Balakrishnan"
    ]
  },
  {
    "title": "Feedforward Neural Networks for Caching: Enough or Too Much?",
    "text": "Feedforward Neural Networks for Caching: Enough or Too Much?. We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
    "pdf_link": "http://arxiv.org/pdf/1810.06930v1.pdf",
    "authors": [
      "Vladyslav Fedchenko",
      "Giovanni Neglia",
      "Bruno Ribeiro"
    ]
  },
  {
    "title": "Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise",
    "text": "Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise. Neural Ordinary Differential Equation (Neural ODE) has been proposed as a\ncontinuous approximation to the ResNet architecture. Some commonly used\nregularization mechanisms in discrete neural networks (e.g. dropout, Gaussian\nnoise) are missing in current Neural ODE networks. In this paper, we propose a\nnew continuous neural network framework called Neural Stochastic Differential\nEquation (Neural SDE) network, which naturally incorporates various commonly\nused regularization mechanisms based on random noise injection. Our framework\ncan model various types of noise injection frequently used in discrete networks\nfor regularization purpose, such as dropout and additive/multiplicative noise\nin each block. We provide theoretical analysis explaining the improved\nrobustness of Neural SDE models against input perturbations/adversarial\nattacks. Furthermore, we demonstrate that the Neural SDE network can achieve\nbetter generalization than the Neural ODE and is more resistant to adversarial\nand non-adversarial input perturbations.",
    "pdf_link": "http://arxiv.org/pdf/1906.02355v1.pdf",
    "authors": [
      "Xuanqing Liu",
      "Tesi Xiao",
      "Si Si",
      "Qin Cao",
      "Sanjiv Kumar",
      "Cho-Jui Hsieh"
    ]
  },
  {
    "title": "The Representation Theory of Neural Networks",
    "text": "The Representation Theory of Neural Networks. In this work, we show that neural networks can be represented via the\nmathematical theory of quiver representations. More specifically, we prove that\na neural network is a quiver representation with activation functions, a\nmathematical object that we represent using a network quiver. Also, we show\nthat network quivers gently adapt to common neural network concepts such as\nfully-connected layers, convolution operations, residual connections, batch\nnormalization, pooling operations and even randomly wired neural networks. We\nshow that this mathematical representation is by no means an approximation of\nwhat neural networks are as it exactly matches reality. This interpretation is\nalgebraic and can be studied with algebraic methods. We also provide a quiver\nrepresentation model to understand how a neural network creates representations\nfrom the data. We show that a neural network saves the data as quiver\nrepresentations, and maps it to a geometrical space called the moduli space,\nwhich is given in terms of the underlying oriented graph of the network, i.e.,\nits quiver. This results as a consequence of our defined objects and of\nunderstanding how the neural network computes a prediction in a combinatorial\nand algebraic way. Overall, representing neural networks through the quiver\nrepresentation theory leads to 9 consequences and 4 inquiries for future\nresearch that we believe are of great interest to better understand what neural\nnetworks are and how they work.",
    "pdf_link": "http://arxiv.org/pdf/2007.12213v2.pdf",
    "authors": [
      "Marco Antonio Armenta",
      "Pierre-Marc Jodoin"
    ]
  },
  {
    "title": "Deep Kronecker neural networks: A general framework for neural networks\n  with adaptive activation functions",
    "text": "Deep Kronecker neural networks: A general framework for neural networks\n  with adaptive activation functions. We propose a new type of neural networks, Kronecker neural networks (KNNs),\nthat form a general framework for neural networks with adaptive activation\nfunctions. KNNs employ the Kronecker product, which provides an efficient way\nof constructing a very wide network while keeping the number of parameters low.\nOur theoretical analysis reveals that under suitable conditions, KNNs induce a\nfaster decay of the loss than that by the feed-forward networks. This is also\nempirically verified through a set of computational examples. Furthermore,\nunder certain technical assumptions, we establish global convergence of\ngradient descent for KNNs. As a specific case, we propose the Rowdy activation\nfunction that is designed to get rid of any saturation region by injecting\nsinusoidal fluctuations, which include trainable parameters. The proposed Rowdy\nactivation function can be employed in any neural network architecture like\nfeed-forward neural networks, Recurrent neural networks, Convolutional neural\nnetworks etc. The effectiveness of KNNs with Rowdy activation is demonstrated\nthrough various computational experiments including function approximation\nusing feed-forward neural networks, solution inference of partial differential\nequations using the physics-informed neural networks, and standard deep\nlearning benchmark problems using convolutional and fully-connected neural\nnetworks.",
    "pdf_link": "http://arxiv.org/pdf/2105.09513v2.pdf",
    "authors": [
      "Ameya D. Jagtap",
      "Yeonjong Shin",
      "Kenji Kawaguchi",
      "George Em Karniadakis"
    ]
  },
  {
    "title": "Can a powerful neural network be a teacher for a weaker neural network?",
    "text": "Can a powerful neural network be a teacher for a weaker neural network?. The transfer learning technique is widely used to learning in one context and\napplying it to another, i.e. the capacity to apply acquired knowledge and\nskills to new situations. But is it possible to transfer the learning from a\ndeep neural network to a weaker neural network? Is it possible to improve the\nperformance of a weak neural network using the knowledge acquired by a more\npowerful neural network? In this work, during the training process of a weak\nnetwork, we add a loss function that minimizes the distance between the\nfeatures previously learned from a strong neural network with the features that\nthe weak network must try to learn. To demonstrate the effectiveness and\nrobustness of our approach, we conducted a large number of experiments using\nthree known datasets and demonstrated that a weak neural network can increase\nits performance if its learning process is driven by a more powerful neural\nnetwork.",
    "pdf_link": "http://arxiv.org/pdf/2005.00393v2.pdf",
    "authors": [
      "Nicola Landro",
      "Ignazio Gallo",
      "Riccardo La Grassa"
    ]
  },
  {
    "title": "Message Passing Variational Autoregressive Network for Solving\n  Intractable Ising Models",
    "text": "Message Passing Variational Autoregressive Network for Solving\n  Intractable Ising Models. Many deep neural networks have been used to solve Ising models, including\nautoregressive neural networks, convolutional neural networks, recurrent neural\nnetworks, and graph neural networks. Learning a probability distribution of\nenergy configuration or finding the ground states of a disordered, fully\nconnected Ising model is essential for statistical mechanics and NP-hard\nproblems. Despite tremendous efforts, a neural network architecture with the\nability to high-accurately solve these fully connected and extremely\nintractable problems on larger systems is still lacking. Here we propose a\nvariational autoregressive architecture with a message passing mechanism, which\ncan effectively utilize the interactions between spin variables. The new\nnetwork trained under an annealing framework outperforms existing methods in\nsolving several prototypical Ising spin Hamiltonians, especially for larger\nspin systems at low temperatures. The advantages also come from the great\nmitigation of mode collapse during the training process of deep neural\nnetworks. Considering these extremely difficult problems to be solved, our\nmethod extends the current computational limits of unsupervised neural networks\nto solve combinatorial optimization problems.",
    "pdf_link": "http://arxiv.org/pdf/2404.06225v1.pdf",
    "authors": [
      "Qunlong Ma",
      "Zhi Ma",
      "Jinlong Xu",
      "Hairui Zhang",
      "Ming Gao"
    ]
  },
  {
    "title": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy",
    "text": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy. The evolution of a deep neural network trained by the gradient descent can be\ndescribed by its neural tangent kernel (NTK) as introduced in [20], where it\nwas proven that in the infinite width limit the NTK converges to an explicit\nlimiting kernel and it stays constant during training. The NTK was also\nimplicit in some other recent papers [6,13,14]. In the overparametrization\nregime, a fully-trained deep neural network is indeed equivalent to the kernel\nregression predictor using the limiting NTK. And the gradient descent achieves\nzero training loss for a deep overparameterized neural network. However, it was\nobserved in [5] that there is a performance gap between the kernel regression\nusing the limiting NTK and the deep neural networks. This performance gap is\nlikely to originate from the change of the NTK along training due to the finite\nwidth effect. The change of the NTK along the training is central to describe\nthe generalization features of deep neural networks.\n  In the current paper, we study the dynamic of the NTK for finite width deep\nfully-connected neural networks. We derive an infinite hierarchy of ordinary\ndifferential equations, the neural tangent hierarchy (NTH) which captures the\ngradient descent dynamic of the deep neural network. Moreover, under certain\nconditions on the neural network width and the data set dimension, we prove\nthat the truncated hierarchy of NTH approximates the dynamic of the NTK up to\narbitrary precision. This description makes it possible to directly study the\nchange of the NTK for deep neural networks, and sheds light on the observation\nthat deep neural networks outperform kernel regressions using the corresponding\nlimiting NTK.",
    "pdf_link": "http://arxiv.org/pdf/1909.08156v1.pdf",
    "authors": [
      "Jiaoyang Huang",
      "Horng-Tzer Yau"
    ]
  },
  {
    "title": "Novel Kernel Models and Exact Representor Theory for Neural Networks\n  Beyond the Over-Parameterized Regime",
    "text": "Novel Kernel Models and Exact Representor Theory for Neural Networks\n  Beyond the Over-Parameterized Regime. This paper presents two models of neural-networks and their training\napplicable to neural networks of arbitrary width, depth and topology, assuming\nonly finite-energy neural activations; and a novel representor theory for\nneural networks in terms of a matrix-valued kernel. The first model is exact\n(un-approximated) and global, casting the neural network as an elements in a\nreproducing kernel Banach space (RKBS); we use this model to provide tight\nbounds on Rademacher complexity. The second model is exact and local, casting\nthe change in neural network function resulting from a bounded change in\nweights and biases (ie. a training step) in reproducing kernel Hilbert space\n(RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model\nprovides insight into model adaptation through tight bounds on Rademacher\ncomplexity of network adaptation. We also prove that the neural tangent kernel\n(NTK) is a first-order approximation of the LiNK kernel. Finally, and noting\nthat the LiNK does not provide a representor theory for technical reasons, we\npresent an exact novel representor theory for layer-wise neural network\ntraining with unregularized gradient descent in terms of a local-extrinsic\nneural kernel (LeNK). This representor theory gives insight into the role of\nhigher-order statistics in neural network training and the effect of kernel\nevolution in neural-network kernel models. Throughout the paper (a) feedforward\nReLU networks and (b) residual networks (ResNet) are used as illustrative\nexamples.",
    "pdf_link": "http://arxiv.org/pdf/2405.15254v1.pdf",
    "authors": [
      "Alistair Shilton",
      "Sunil Gupta",
      "Santu Rana",
      "Svetha Venkatesh"
    ]
  },
  {
    "title": "Descriptive complexity for neural networks via Boolean networks",
    "text": "Descriptive complexity for neural networks via Boolean networks. We investigate the descriptive complexity of a class of neural networks with\nunrestricted topologies and piecewise polynomial activation functions. We\nconsider the general scenario where the networks run for an unlimited number of\nrounds and floating-point numbers are used to simulate reals. We characterize\nthese neural networks with a recursive rule-based logic for Boolean networks.\nIn particular, we show that the sizes of the neural networks and the\ncorresponding Boolean rule formulae are polynomially related. In fact, in the\ndirection from Boolean rules to neural networks, the blow-up is only linear.\nOur translations result in a time delay, which is the number of rounds that it\ntakes for an object's translation to simulate a single round of the object. In\nthe translation from neural networks to Boolean rules, the time delay of the\nresulting formula is polylogarithmic in the neural network size. In the\nconverse translation, the time delay of the neural network is linear in the\nformula size. As a corollary, by restricting our logic, we obtain a similar\ncharacterization for classical feedforward neural networks. We also obtain\ntranslations between the rule-based logic for Boolean networks, the\ndiamond-free fragment of modal substitution calculus and a class of recursive\nBoolean circuits where the number of input and output gates match. Ultimately,\nour translations offer a method of translating a given neural network into an\nequivalent neural network with different activation functions, including linear\nactivation functions!",
    "pdf_link": "http://arxiv.org/pdf/2308.06277v3.pdf",
    "authors": [
      "Veeti Ahvonen",
      "Damian Heiman",
      "Antti Kuusisto"
    ]
  },
  {
    "title": "Feature Weight Tuning for Recursive Neural Networks",
    "text": "Feature Weight Tuning for Recursive Neural Networks. This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.",
    "pdf_link": "http://arxiv.org/pdf/1412.3714v2.pdf",
    "authors": [
      "Jiwei Li"
    ]
  }
]